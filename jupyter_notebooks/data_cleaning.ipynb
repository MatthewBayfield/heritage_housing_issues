{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data cleaning**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "**Perform Business requirement 2 user story task: Data cleaning and preparation ML tasks**\n",
        "* Find and correct if necessary invalid data.\n",
        "* Handle outliers.\n",
        "* Split dataset in to train and test subsets.\n",
        "* Impute missing data.\n",
        "* Save cleaned train and test datasets\n",
        "* Determine some of the steps of the data cleaning and feature engineering pipeline.\n",
        "\n",
        "## Inputs\n",
        "* house prices dataset: outputs/datasets/collection/house_prices.csv\n",
        "\n",
        "## Outputs\n",
        "* cleaned train set: outputs/datasets/ml/cleaned/train_set.csv\n",
        "* cleaned test set: outputs/datasets/ml/cleaned/test_set.csv\n",
        "* Pickled outlier indices list: outputs/ml/outlier_indices.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "Working directory changed to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load house price dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "house_prices_df = pd.read_csv(filepath_or_buffer='outputs/datasets/collection/house_prices.csv')\n",
        "house_prices_df.dtypes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We know from the data collection notebook, that there are no duplicates in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Invalid data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data types"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspection of the data types for each variable, except for shows no discrepancies from the expectation for each variable's suitable data type."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Value ranges"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking the values for each variable are within the numeric valid range or equal to one of the categorical options, as indicated in the datasets metadata."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**First for numeric variables**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_house_prices_df = house_prices_df.select_dtypes(exclude=['object'])\n",
        "numeric_house_prices_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def check_value_ranges(variable, value_range):\n",
        "    \"\"\"\n",
        "    Checks whether the non-missing values for a 'house_prices_df' numeric variable are in the valid variable range.\n",
        "\n",
        "    Args:\n",
        "        variable (str): name of variable.\n",
        "        value_range (list): [minimum value, maximum value].\n",
        "    \n",
        "    Returns a boolean indicating whether all values of the variable are in the valid range.\n",
        "\n",
        "    \"\"\"\n",
        "    variable_series = house_prices_df[variable]\n",
        "    # drop missing data\n",
        "    variable_series.dropna(inplace=True)\n",
        "    result_series = variable_series[variable_series <= value_range[1]]\n",
        "    result_series = result_series >= value_range[0]\n",
        "    return result_series.size == variable_series.size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('|Variable|Valid range|Data in valid range|')\n",
        "variable_value_ranges = {'1stFlrSF': [334, 4692], '2ndFlrSF': [0, 2065], 'BedroomAbvGr': [0, 8], 'BsmtFinSF1': [0, 5644],\n",
        "                         'BsmtUnfSF': [0, 2336], 'TotalBsmtSF': [0, 6110], 'GarageArea': [0, 1418], 'GarageYrBlt': [1900, 2010],\n",
        "                         'GrLivArea': [334, 5642], 'LotArea': [1300, 215245], 'LotFrontage': [21, 313], 'MasVnrArea': [0, 1600],\n",
        "                         'EnclosedPorchSF': [0, 286], 'OpenPorchSF': [0, 547], 'OverallCond': [1, 10], 'OverallQual': [1,10],\n",
        "                         'WoodDeckSF': [0, 736], 'YearBuilt': [1872, 2010], 'YearRemodAdd': [1950, 2010], 'SalePrice': [34900, 755000]}\n",
        "\n",
        "for variable in numeric_house_prices_df.columns:\n",
        "    print(f'{variable}|', f'{variable_value_ranges[variable]}|', check_value_ranges(variable, variable_value_ranges[variable]))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All non-missing values are in the valid range for each numeric variable."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Now for categorical variables**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_house_prices_df = house_prices_df.select_dtypes(include=['object'])\n",
        "categorical_house_prices_df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# include NaN as a valid value \n",
        "result_df = categorical_house_prices_df.isin({'BsmtExposure': ['Gd', 'Av', 'Mn', 'No', 'None', np.nan], 'BsmtFinType1': ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None', np.nan],\n",
        "                                  'GarageFinish': ['Fin', 'RFn', 'Unf', 'None', np.nan], 'KitchenQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po', np.nan]})\n",
        "for col in result_df.columns:\n",
        "    print(result_df[col].value_counts())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All values are valid for the categorical variables (allowing for missing data)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outliers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Determined from the significant features EDA notebook that for the most significant continuous numeric features in relation to sale price, there were several instances whose vector components were outliers in at least 50% of the continuous features, thus making them more likely multivariate outliers. What's more it was discovered that the components of these instances corresponded to the extremest outliers for multiple features, supporting the idea of a correlation between the number of features for which an instance's component is an outlier, and the extremity of the outliers."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Outliers for each feature (using the whole dataset) were determined using the IQR method, and the indices of the outliers tracked and counted to determine if the same instance gave rise to outliers for other features. It is common that the dataset is first split into train and test sets before handling outliers, perhaps using a transformer such as winsorize; the idea being to minimise the risk of data leakage. However arguably an outlier in the whole data set (at least the most extreme ones) will still be an outlier in a sample of the distribution (if it is present). Also it could be argued that such values if particularly extreme, and depending on the context of the dataset and business aims, offer no value, and potentially impact the ML algorithms. Therefore for this dataset the outliers will be trimmed from the whole dataset. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Adding the outlier related functions from the significant feature EDA notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# taken from significant_feature_EDA.ipynb\n",
        "def locate_single_feature_outliers(feature, df):\n",
        "    \"\"\"\n",
        "    Locates outliers for a feature in a dataframe (containing only numeric features) using the IQR method.\n",
        "\n",
        "    Args:\n",
        "        feature (str): the feature name.\n",
        "        df: dataframe containing the feature.\n",
        "\n",
        "    Returns a list of indices corresponding to the dataframe indices of the outliers.\n",
        "    \"\"\"\n",
        "    sample = df[feature]\n",
        "    mean = sample.mean()\n",
        "    SD = sample.std()\n",
        "    Q1 = sample.quantile(q=0.25)\n",
        "    Q3 = sample.quantile(q=0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    def return_outliers(instance):\n",
        "        return instance > IQR*1.5 + Q3 or instance < Q1 - 1.5*IQR\n",
        "    result = sample.apply(func=return_outliers)\n",
        "    return result[result == True].index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# taken from significant_feature_EDA.ipynb\n",
        "def locate_all_feature_outliers(df):\n",
        "    \"\"\"\n",
        "    Amalgamates into a single list, the dataframe (containing only numeric features) indices corresponding to all outliers of features in a dataframe.\n",
        "\n",
        "    args:\n",
        "        df: dataframe containing numeric features.\n",
        "\n",
        "    Returns a list. It contains a series with index corresponding to the index of an outlier, and a column value\n",
        "    corresponding to the number of times the instance is a common outlier across all features. Also contains\n",
        "    a value_counts series for the series; finally contains a float for the number of features in the dataframe.\n",
        "    \"\"\"\n",
        "    outlier_indices = []\n",
        "    for col in df.columns:\n",
        "        found_ouliers = locate_single_feature_outliers(col, df)\n",
        "        outlier_indices.extend(found_ouliers)\n",
        "    index_freq = np.array(outlier_indices)\n",
        "    index_count = np.unique(index_freq, return_counts=True)\n",
        "    index_count_series = pd.Series(data=index_count[1], index=index_count[0]).sort_values(ascending=False)\n",
        "    return [index_count_series, index_count_series.value_counts().sort_values(), df.columns.size]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rediscovering the outlier instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "continuous_numeric_features = ['1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'GarageArea', 'GrLivArea',\n",
        "                               'LotArea',\n",
        "                               'LotFrontage',\n",
        "                               'MasVnrArea',\n",
        "                               'OpenPorchSF',\n",
        "                               'TotalBsmtSF']\n",
        "outlier_series, outlier_series_unique_count, total_feature_num = locate_all_feature_outliers(house_prices_df[continuous_numeric_features])\n",
        "print('\\n','Instances whose component values correspond to potential outliers in more than 50% of continuous numeric features:')\n",
        "house_prices_df[continuous_numeric_features].loc[outlier_series[outlier_series > 5].index.tolist()]   "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Removing these instances from the whole dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outlier_indices = house_prices_df.loc[outlier_series[outlier_series > 5].index.tolist()].index.tolist()\n",
        "outlier_indices"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the outlier_indices list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    path = os.path.join(os.getcwd(), 'outputs/ml/')\n",
        "    os.makedirs(path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import joblib\n",
        "    path = os.path.join(os.getcwd(), 'outputs/ml/outlier_indices.pkl')\n",
        "    joblib.dump(outlier_indices, path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dropping the instances corresponding to the outliers from the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_prices_df.drop(labels=outlier_indices, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_set_df, test_set_df) = train_test_split(house_prices_df, test_size=0.25, random_state=30)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling missing data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During the sale price correlation study, missing values in the whole dataset were imputed, using a combination of the KNN (k-nearest neighbors) imputer for numeric features, and an equal value frequency imputer method for categorical features. The idea being to attempt to realistically replicate real data and to not distort the distributions. Comparing the distributions before and after revealed no significant distortions.\n",
        "\n",
        "Therefore the same methods will be used again, but this time applied separately to the train and test subsets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identifying missing values in the test and train subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_missing_data_df = train_set_df.loc[:, train_set_df.isna().any()]\n",
        "print(train_set_missing_data_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_missing_data_df = test_set_df.loc[:, test_set_df.isna().any()]\n",
        "print(test_set_missing_data_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* So can see that the target in both test and train subsets has no missing values.\n",
        "* Can see, as was already known, that there are missing values in both feature train and test subsets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Imputing missing values in train and test subsets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_numeric_df = train_set_df.select_dtypes(exclude='object')\n",
        "train_set_non_numeric_df = train_set_df.select_dtypes(include='object')\n",
        "test_set_numeric_df = test_set_df.select_dtypes(exclude='object')\n",
        "test_set_non_numeric_df = test_set_df.select_dtypes(include='object')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First for numeric features:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating function to carry out imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def numeric_feature_missing_value_KNNImputer(numeric_df, transform_df, missing_data_df):\n",
        "    \"\"\"\n",
        "    fit_transforms numeric features of a dataset using the KNNImputer. Produces before and after histograms.\n",
        "\n",
        "    Args:\n",
        "        numeric_df: dataframe containing all of the numeric features only.\n",
        "        transform_df: dataframe which is updated with imputed values, and from which the numeric features originate.\n",
        "        missing_data_df: dataframe containing columns (all types) with missing data only.\n",
        "    \"\"\"\n",
        "    # plotting distributions for numeric variables with missing data\n",
        "    counter = 0\n",
        "    imputed_columns = []\n",
        "    while counter < len(numeric_df.columns):\n",
        "        if numeric_df.iloc[:, counter].name in missing_data_df.columns:\n",
        "            fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(10,4))\n",
        "            sns.histplot(x=numeric_df.iloc[:, counter], ax=ax[0])\n",
        "            imputed_columns.append(numeric_df.iloc[:, counter].name)\n",
        "        counter += 1\n",
        "    \n",
        "    # Imputing the missing values for required columns\n",
        "    imputer = KNNImputer()\n",
        "    imputer.set_output(transform='pandas')\n",
        "    numeric_df = imputer.fit_transform(numeric_df)\n",
        "    # check missing values have been replaced\n",
        "    print('|feature|Number of missing values|')\n",
        "    print(numeric_df.isna().sum())\n",
        "    # converting back values to integers for integer features (consequence of taking the mean during KNN imputing)\n",
        "    for col in ['BedroomAbvGr','GarageYrBlt','OverallCond','OverallQual','YearBuilt', 'YearRemodAdd']:\n",
        "        numeric_df[col] = numeric_df[col].round()\n",
        "\n",
        "    # transforming parent dataframe\n",
        "    transform_df[numeric_df.columns.values] = numeric_df\n",
        "\n",
        "    # plotting distributions after imputation on same figures, for visual comparison\n",
        "    print('\\n', 'Distributions before and after missing value imputation')\n",
        "    for fig in plt.get_fignums():\n",
        "        sns.histplot(x=numeric_df[imputed_columns[fig - 1]], ax=plt.figure(fig).get_axes()[1])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train set missing values imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_feature_missing_value_KNNImputer(train_set_numeric_df.drop('SalePrice', axis=1), train_set_df, train_set_missing_data_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "test set missing values imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_feature_missing_value_KNNImputer(test_set_numeric_df.drop('SalePrice', axis=1), test_set_df, test_set_missing_data_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The choice was made to fit the imputer to the train and test sets separately, as opposed to fitting only to the train set and transforming both sets after. The rationale for the alternative is that you should only use information learned from the train set, to transform in the same way the train and test sets. Whilst in many cases it would be a clear example of data leakage if the test set was used to influence the train set, it is not clear whether modifying each subset independently impacts the model's performance, at least for transformers that are sample specific (e.g. if they use the location of nearest neighbors).\n",
        "\n",
        "The KNN imputer uses the k-nearest neighbors to calculate the missing values. During fitting it identifies, as far as I can tell, the nearest neighbors by their locations, for all instances in the dataset (missing or not). So if a different dataset is used to fit the imputer, to that which is transformed, then the relative locations of the nearest neighbors of a missing value in the transformed dataset, are determined by the nearest neighbors of the identically located value (missing or not) in the fitted dataset. The missing value in the transformed dataset is then calculated using the values located at the positions of the nearest neighbors in the fitted dataset.\n",
        "\n",
        "Consequently if the test set was transformed using the train set fitted imputer, then the values used to replace the missing values will have little or no relationship to the instance's other feature values. This then seems to defeat the purpose of using the KNN imputer to more realistically replace any missing values with something approximating the true value --- assuming a relationship exists between different features as well as the target. If the goal is to predict the sale price based on an assumed relationship to the features, which themselves may be related, then it makes sense to impute the test set in a way which is more likely to preserve any relationship, which has also been hopefully preserved in the train set using the KNN imputer independently."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the categorical/non-numeric features:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating function to carry out imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def equal_frequency_imputer_categorical_features(categorical_df, transform_df, missing_data_df):\n",
        "    \"\"\"\n",
        "    Imputes missing values for categorical features using an equal frequency value replacement method. Produces before and after count plots.\n",
        "\n",
        "    The missing values are individually replaced in sequence by repeatedly cycling through one of the possible values.\n",
        "\n",
        "    Args:\n",
        "        categorical_df: dataframe containing all of the categorical features only.\n",
        "        transform_df: dataframe which is updated with imputed values, and from which the categorical features originate.\n",
        "        missing_data_df: dataframe containing columns (all types) with missing data only.\n",
        "        \n",
        "    \"\"\"\n",
        "    # plotting distributions for categorical variables with missing data\n",
        "    counter = 0\n",
        "    imputed_columns = []\n",
        "    while counter < len(categorical_df.columns):\n",
        "        if categorical_df.iloc[:, counter].name in missing_data_df.columns:\n",
        "            fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(10,4))\n",
        "            sns.countplot(x=categorical_df.iloc[:, counter], ax=ax[0])\n",
        "            imputed_columns.append(categorical_df.iloc[:, counter].name)\n",
        "        counter += 1\n",
        "\n",
        "    # imputing the missing values\n",
        "    for col in imputed_columns:\n",
        "        number_of_nans = categorical_df[col].loc[categorical_df[col].isna() == True].size\n",
        "        unique_values = categorical_df[col].unique()\n",
        "        index_no = 0\n",
        "        while number_of_nans > 0:\n",
        "            if index_no + 1 >= unique_values.size:\n",
        "                index_no = 0\n",
        "            categorical_df[col].fillna(value=unique_values[index_no], limit=1, inplace=True)\n",
        "            transform_df[col].fillna(value=unique_values[index_no], limit=1, inplace=True)\n",
        "            index_no += 1\n",
        "            number_of_nans = categorical_df[col].loc[categorical_df[col].isna() == True].size\n",
        "\n",
        "    # checking the missing values have be replaced\n",
        "    print('|feature|Number of missing values|')\n",
        "    print(categorical_df.isna().sum())\n",
        "\n",
        "    # plotting the distributions on the same figures after imputation for comparison\n",
        "    print('\\n', 'Distributions before and after missing value imputation')\n",
        "    for fig in plt.get_fignums():\n",
        "        sns.countplot(x=categorical_df[imputed_columns[fig - 1]], ax=plt.figure(fig).get_axes()[1])\n",
        "            \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imputing the missing values for the categorical features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "equal_frequency_imputer_categorical_features(train_set_non_numeric_df, train_set_df, train_set_missing_data_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "equal_frequency_imputer_categorical_features(test_set_non_numeric_df, test_set_df, test_set_missing_data_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the categorical features it is more difficult to replace the missing value with a true value, and so it was decided to preserve the distribution shape. By cycling through the missing values and replacing them in sequence with one of the possible values in an equally distributed fashion, the hope is to potentially only disrupt the relationships between these features and the target somewhat randomly, and thus hopefully only causing some noise, without altering the direction of any correlation. Fortunately for the categorical features, only a small amount of missing values were present."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Final check that all missing values have been replaced**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('train_set')\n",
        "print('|feature|Number of missing values|')\n",
        "print(train_set_df.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('test_set')\n",
        "print('|feature|Number of missing values|')\n",
        "print(test_set_df.isna().sum())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving cleaned datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    path = os.path.join(os.getcwd(), 'outputs/datasets/ml/cleaned/')\n",
        "    os.makedirs(path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    train_set_df.to_csv(os.path.join(path, 'train_set.csv'), index=False)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    test_set_df.to_csv(os.path.join(path, 'test_set.csv'), index=False)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating custom transformers for pipelines"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating transformer for imputing the missing values of categorical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class EqualFrequencyImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Imputes missing values for categorical features using an equal frequency replacement with possible feature values.\n",
        "    \"\"\"\n",
        "    def fit(self, x, y):\n",
        "        \"\"\"\n",
        "        No fitting is performed. The equal_frequency_imputer_categorical_features funnction is defined.\n",
        "        \"\"\"\n",
        "        def equal_frequency_imputer_categorical_features(categorical_df, transform_df, missing_data_df):\n",
        "            \"\"\"\n",
        "            Imputes missing values for categorical features using an equal frequency value replacement method. Produces before and after count plots.\n",
        "\n",
        "            The missing values are individually replaced in sequence by repeatedly cycling through one of the possible values.\n",
        "\n",
        "            Args:\n",
        "                categorical_df: dataframe containing all of the categorical features only.\n",
        "                transform_df: dataframe which is updated with imputed values, and from which the categorical features originate.\n",
        "                missing_data_df: dataframe containing columns (all types) with missing data only.\n",
        "                \n",
        "            \"\"\"\n",
        "            counter = 0\n",
        "            imputed_columns = []\n",
        "            while counter < len(categorical_df.columns):\n",
        "                if categorical_df.iloc[:, counter].name in missing_data_df.columns:\n",
        "                    imputed_columns.append(categorical_df.iloc[:, counter].name)\n",
        "                counter += 1\n",
        "\n",
        "            for col in imputed_columns:\n",
        "                number_of_nans = categorical_df[col].loc[categorical_df[col].isna() == True].size\n",
        "                unique_values = categorical_df[col].unique()\n",
        "                index_no = 0\n",
        "                while number_of_nans > 0:\n",
        "                    if index_no + 1 >= unique_values.size:\n",
        "                        index_no = 0\n",
        "                    categorical_df[col].fillna(value=unique_values[index_no], limit=1, inplace=True)\n",
        "                    transform_df[col].fillna(value=unique_values[index_no], limit=1, inplace=True)\n",
        "                    index_no += 1\n",
        "                    number_of_nans = categorical_df[col].loc[categorical_df[col].isna() == True].size\n",
        "\n",
        "        self.equal_frequency_imputer = equal_frequency_imputer_categorical_features\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        \"\"\"\n",
        "        Transform the features by applying the equal frequency imputer function.\n",
        "        \"\"\"\n",
        "        self.categorical_df = x.select_dtypes(include='object')\n",
        "        self.missing_data_df = x.loc[:, x.isna().any()]\n",
        "\n",
        "        self.equal_frequency_imputer(self.categorical_df, x, self.missing_data_df)\n",
        "\n",
        "        return x\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating custom transformer for imputing missing values for numeric features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transformer is trivial, but needed because of the desire to fit and transform train and test sets independently with the KNNImputer. The motivation for this was discussed earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IndependentKNNImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Imputes missing values for numerical features using a KNNImputer.\n",
        "    \n",
        "    Allows independent fitting and transforming for train and test sets.\n",
        "    \"\"\"\n",
        "    def fit(self, x, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        \"\"\"\n",
        "        Transforms using a KNNImputer.\n",
        "        \"\"\"\n",
        "        self.numeric_df = x.select_dtypes(exclude='object')\n",
        "        imputer = KNNImputer()\n",
        "        imputer.set_output(transform='pandas')\n",
        "        imputer.fit(self.numeric_df)\n",
        "        x[self.numeric_df.columns] = imputer.transform(self.numeric_df)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data cleaning and feature engineering pipeline steps**\n",
        "\n",
        "* Will impute missing values for numeric features using a custom transformer IndependentKNNImputer.\n",
        "* Will impute missing values for the categorical features using a custom transformer: EqualFrequencyImputer\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
