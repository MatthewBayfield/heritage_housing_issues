{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering, Selection, and Scaling**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "**Perform Business requirement 2 user story task: Feature engineering, selection, and scaling ML tasks**\n",
        "* Perform categorical encoding for categorical features.\n",
        "* Perform feature selection to distill the most significant features, and also remove redundant features.\n",
        "* Carry out feature scaling/transformations to normalise/standardise the distributions of remaining features.\n",
        "* Determine some of the steps to be included in the data cleaning and feature engineering pipeline, as well as the modelling pipeline.\n",
        "\n",
        "## Inputs\n",
        "* cleaned train set: outputs/datasets/ml/cleaned/train_set.csv\n",
        "* cleaned test set: outputs/datasets/ml/cleaned/test_set.csv\n",
        "\n",
        "## Outputs\n",
        "* Specified data cleaning and engineering pipeline steps to include.\n",
        "* Specified modelling pipeline steps to include.\n",
        "* CompositeSelectKBest custom transformer for use as a feature selection step in the modelling pipeline. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "Working directory changed to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load cleaned train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_set_df = pd.read_csv(filepath_or_buffer='outputs/datasets/ml/cleaned/train_set.csv')\n",
        "test_set_df = pd.read_csv(filepath_or_buffer='outputs/datasets/ml/cleaned/test_set.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical feature encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the sale price correlation study notebook, the categorical features were encoded using an ordinal encoder; this was deemed most suitable since all the cateogrical features are ordinal, with an obvious ordering based around a rating.\n",
        "\n",
        "The exact same encoding will be used for the cleaned train and test sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_categorical_df = train_set_df.select_dtypes(include='object')\n",
        "print(train_set_categorical_df.columns)\n",
        "test_set_categorical_df = test_set_df.select_dtypes(include='object')\n",
        "test_set_categorical_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Designating the ordered categories\n",
        "bsmt_fin_type1_cat = np.array(list(reversed(['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'])))\n",
        "bsmt_exposure_cat = np.array(['None', 'No', 'Mn', 'Av', 'Gd'])\n",
        "garage_finish_cat = np.array(['None', 'Unf', 'RFn', 'Fin'])\n",
        "kitchen_quality_cat = np.array(['Po', 'Fa', 'TA', 'Gd', 'Ex'])\n",
        "\n",
        "categories = [bsmt_exposure_cat, bsmt_fin_type1_cat, garage_finish_cat, kitchen_quality_cat]\n",
        "encoder = OrdinalEncoder(categories=categories, dtype='int64')\n",
        "encoder.set_output(transform='pandas')\n",
        "\n",
        "# fitting and transforming each set\n",
        "train_set_categorical_df = encoder.fit_transform(X=train_set_categorical_df)\n",
        "train_set_df[train_set_categorical_df.columns] = train_set_categorical_df\n",
        "print(train_set_df[train_set_categorical_df.columns].head())\n",
        "test_set_categorical_df = encoder.transform(X=test_set_categorical_df)\n",
        "test_set_df[test_set_categorical_df.columns] = test_set_categorical_df\n",
        "test_set_df[test_set_categorical_df.columns].head()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial feature selection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It was decided to perform an initial feature selection before feature scaling, for primarily two reasons. Firstly there is quite a large number of features (23), for which performing feature scaling would be time consuming, as well as pointless for several features which have much less significance with regard to the target; this was already discovered in the sales price correlation study notebook, where a group of significant features was generated using correlation tests and PPS's applied to the whole dataset. The second reason is that as this initial selection will not use a ML model to establish the significant features, scaling is not important. \n",
        "\n",
        "Instead a similar approach to that used in the sales price correlation study involving correlation tests, but with the 'selectKBest method', will be used --- again this is not affected by scaling. \n",
        "\n",
        "After this is performed, feature-feature correlations will be assessed, and one of the features of any strongly correlated group will be dropped in order to reduce redundancy; which was seen to exist in the significant feature EDA notebook. A decision tree algorithm will be used, as this is unaffected by scaling.\n",
        "\n",
        "Feature scaling will then be performed on the most significant features, before a further feature selection will occur that relies on scale affected ML models. Since scaling has been performed, this should pose no issues."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SelectKBest feature selection**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SelectKBest feature selection will be performed twice, once using the Spearman correlation test, then using the phi_k correlation test."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the score functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pingouin as pg\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "def spearman_score_function(feature_array, target_array):\n",
        "    \"\"\"\n",
        "    Calculates spearman correlation coefficients for the train or test set.\n",
        "\n",
        "    To be used as the score function in the sklearn.feature_selection.SelectKBest function.\n",
        "\n",
        "    Args:\n",
        "        feature_array = array-like, containing train or test set feature data.\n",
        "        target_array = array-like, containing train or test set target data\n",
        "\n",
        "    Returns tuple of the spearman r values series, and the spearman p values series for the dataset.\n",
        "    \"\"\"\n",
        "    column_names = train_set_df.columns.to_list()\n",
        "    df = pd.DataFrame(data=feature_array, columns=column_names[0:-1])\n",
        "    df['SalePrice'] = target_array\n",
        "    spearman_df = df.pairwise_corr(columns=['SalePrice'], method='spearman')\n",
        "    print('Spearman coefficients')\n",
        "    print(spearman_df.sort_values(by='r')[['X', 'Y', 'r', 'p-unc']])\n",
        "    return (spearman_df['r'], spearman_df['p-unc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import phik\n",
        "from phik.phik import phik_matrix\n",
        "from phik.report import plot_correlation_matrix\n",
        "from phik.significance import significance_matrix\n",
        "\n",
        "def phik_score_function(feature_array, target_array,):\n",
        "    \"\"\"\n",
        "    Calculates phi_k correlation coefficients for the train or test set.\n",
        "\n",
        "    To be used as the score function in the sklearn.feature_selection.SelectKBest function.\n",
        "\n",
        "    Args:\n",
        "        feature_array = array-like, containing train or test set feature data.\n",
        "        target_array = array-like, containing train or test set target data\n",
        "\n",
        "    Returns tuple of the phi_k correlation values series, and the significance values series for the dataset.\n",
        "    \"\"\"\n",
        "    column_names = train_set_df.columns.to_list()\n",
        "    df = pd.DataFrame(data=feature_array, columns=column_names[0:-1])\n",
        "    df['SalePrice'] = target_array\n",
        "   \n",
        "    phik_matrix_df = phik_matrix(df)[['SalePrice']].drop('SalePrice', axis=0)\n",
        "    significance_matrix_df = significance_matrix(df)[['SalePrice']].drop('SalePrice', axis=0)\n",
        "    print('phi_k coefficients')\n",
        "    print(phik_matrix_df)\n",
        "\n",
        "    return (phik_matrix_df['SalePrice'], significance_matrix_df['SalePrice'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will take the union of filtered out features using each score function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_out_best_features(df):\n",
        "    \"\"\"\n",
        "    Applies a SelectKBest transformer, once with a spearman score function, and then with a phi_k score function.\n",
        "\n",
        "    Args:\n",
        "        df: train or test set dataframe containing features and a target.\n",
        "\n",
        "    Returns a list of the union of the k-best-selected features generated using each score function.\n",
        "    \"\"\"\n",
        "    select_k_best = SelectKBest(score_func=spearman_score_function)\n",
        "    select_k_best.set_output(transform='pandas')\n",
        "    train_set_spearman_transformed_df = select_k_best.fit_transform(df.drop('SalePrice', axis=1), df['SalePrice'])\n",
        "\n",
        "    select_k_best = SelectKBest(score_func=phik_score_function)\n",
        "    select_k_best.set_output(transform='pandas')\n",
        "    train_set_phik_transformed_df = select_k_best.fit_transform(df.drop('SalePrice', axis=1), df['SalePrice'])\n",
        "\n",
        "    return list(set(train_set_spearman_transformed_df.columns.to_list() + train_set_phik_transformed_df.columns.to_list()))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features = filter_out_best_features(train_set_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Best features list**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of features:', len(selected_features))\n",
        "selected_features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can see that most of the selected features match those selected during the sale price correlation study notebook using the whole dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now will manually transform the train set by filtering out only the selected features. Will also filter out the same features from the test set, in order to avoid data leakage by\n",
        "selecting the best features separately using the test set instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_df = train_set_df[selected_features + ['SalePrice']]\n",
        "test_set_df = test_set_df[selected_features + ['SalePrice']]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling redundant features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will select a single feature from any highly correlated feature groups, using the Spearman correlation test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "smart_correlated_transformer = SmartCorrelatedSelection(train_set_df.drop('SalePrice', axis=1).columns.to_list(),\n",
        "                                                        method='spearman', threshold=0.8, selection_method='model_performance',\n",
        "                                                        estimator=estimator, scoring='r2', cv=5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "fitting to train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "smart_correlated_transformer.fit(X=train_set_df, y=train_set_df['SalePrice'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for statement in [smart_correlated_transformer.variables,\n",
        "                  smart_correlated_transformer.correlated_feature_sets_,\n",
        "                  smart_correlated_transformer.features_to_drop_]:\n",
        "                  print(statement)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "transforming train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_df = smart_correlated_transformer.transform(train_set_df)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "transforming test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_df = smart_correlated_transformer.transform(test_set_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature scaling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will aim to attempt to standardise the distributions of the numeric features of the train and test sets. Will try variance-stabilizing-transformers, the standardiser method, or the normalisation method to achieve this, and evaluate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_df.columns.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_numeric_df = train_set_df.drop(['OverallQual', 'KitchenQual', 'GarageFinish', 'SalePrice'], axis=1)\n",
        "test_set_numeric_df  = test_set_df.drop(['OverallQual', 'KitchenQual', 'GarageFinish', 'SalePrice'], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating functions to carry out transformations, and evaluate the transformer effect, for each numeric feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_scaling_transformation(feature_data, transformed_distribution, fig_title=None):\n",
        "    \"\"\"\n",
        "    Plots histograms, box plots, QQ plots to compare feature distributions before and after a transformation.\n",
        "\n",
        "    Args:\n",
        "        feature_data: dataframe containing untransformed numeric features.\n",
        "        transformed_distribution: dataframe containing transformed numeric features.\n",
        "        fig_title (str): Global figure title for a set of subplots.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(ncols=2, nrows=3, figsize=(10,9), tight_layout=True)\n",
        "    fig.suptitle(fig_title)\n",
        "\n",
        "    plot = sns.histplot(data=feature_data, x=feature_data.iloc[:,0], ax=axes[0][0])\n",
        "    plot.set(title='Before')\n",
        "    plot = sns.histplot(data=transformed_distribution, x=transformed_distribution.iloc[:,0], ax=axes[0][1])\n",
        "    plot.set(title='After')\n",
        "\n",
        "    plot = sns.boxplot(data=feature_data, x=feature_data.iloc[:,0], ax=axes[1][0], orient='h')\n",
        "    plot.set(title='Before')\n",
        "    plot = sns.boxplot(data=transformed_distribution, x=transformed_distribution.iloc[:,0], ax=axes[1][1], orient='h')\n",
        "    plot.set(title='After')\n",
        "\n",
        "    plot = pg.qqplot(x=feature_data.iloc[:,0], ax=axes[2][0])\n",
        "    plot.set(title='Before')\n",
        "    plot = pg.qqplot(x=transformed_distribution.iloc[:,0], ax=axes[2][1])\n",
        "    plot.set(title='After')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_all_feature_transformations(transformer, non_zero=False):\n",
        "    \"\"\"\n",
        "    Applies transformer to all numeric features of the train set. Produces plot for before and after comparisons.\n",
        "\n",
        "    Args:\n",
        "        transformer: transformer object.\n",
        "        non-zero (bool): indicates whether only features with non-zero values should be used. \n",
        "    \"\"\"\n",
        "    if not non_zero:\n",
        "        df = transformer.fit_transform(train_set_numeric_df)\n",
        "    else:\n",
        "        df = transformer.fit_transform(train_set_numeric_df.loc[:,(train_set_numeric_df > 0).all().values.tolist()])\n",
        "    \n",
        "    for feature in df.items():\n",
        "        evaluate_scaling_transformation(train_set_numeric_df[[feature[0]]], df[[feature[0]]], feature[0])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Normalisation transformers**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalizer transformer:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This transformer transforms all features simultaneously as it transforms each instance independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "normalizer.set_output(transform='pandas')\n",
        "evaluate_all_feature_transformations(normalizer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MinMaxScaler transformer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_scaler.set_output(transform='pandas')\n",
        "evaluate_all_feature_transformations(min_max_scaler)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Variance-stabilizing-transformers**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Power transformer (exponent=0.5):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.transformation import PowerTransformer\n",
        "\n",
        "power_transformer = PowerTransformer()\n",
        "evaluate_all_feature_transformations(power_transformer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yeo-Johnson-Transformer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.transformation import YeoJohnsonTransformer\n",
        "\n",
        "yeo_johnson_transformer = YeoJohnsonTransformer()\n",
        "evaluate_all_feature_transformations(yeo_johnson_transformer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Further transformations for features with all values > 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log transformer (base e):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.transformation import LogTransformer\n",
        "\n",
        "log_transformer = LogTransformer()\n",
        "evaluate_all_feature_transformations(log_transformer, non_zero=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log transformer (base 10):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.transformation import LogTransformer\n",
        "\n",
        "log_transformer = LogTransformer(base='10')\n",
        "evaluate_all_feature_transformations(log_transformer, non_zero=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Box-Cox-Transformer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.transformation import BoxCoxTransformer\n",
        "\n",
        "box_cox_transformer = BoxCoxTransformer()\n",
        "evaluate_all_feature_transformations(box_cox_transformer, non_zero=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Standardise transformers**:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "StandardScaler transformer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "standard_scaler.set_output(transform='pandas')\n",
        "evaluate_all_feature_transformations(standard_scaler)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|Feature|Observations|\n",
        "|---|---|\n",
        "|EnclosedPorchSF|No real change in normality seen. Will use normaliser to normalise.|\n",
        "|YearBuilt|Improvements in normality seen. Normaliser scales and improves normality.|\n",
        "|TotalBsmtSF|No positive changes in normality. Will use Normaliser to normalise.|\n",
        "|YearRemodAdd|Improvements in normality seen. Normaliser scales and improves normality.|\n",
        "|GarageArea|No positive changes in normality. Will use Normaliser to normalise.|\n",
        "|GrLivArea|large improvements in normality seen. Normaliser favoured as better scaling, and better normality|\n",
        "|2ndFlrSF|No improvements in normality. Use Normaliser to normalise.|\n",
        "|BsmtFinSF1|Improved normality seen. Normaliser preferable.|"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the above observations, all numeric features will be transformed using the sklearn Normaliser transformer."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Transforming numeric features in train and test sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalizer = Normalizer()\n",
        "normalizer.set_output(transform='pandas')\n",
        "\n",
        "train_set_df[train_set_numeric_df.columns.to_list()] = normalizer.fit_transform(train_set_numeric_df)\n",
        "test_set_df[test_set_numeric_df.columns.to_list()] = normalizer.transform(test_set_numeric_df)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical feature scaling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The categorical features will be minimally transformed in order to preserve their encoded value ordering, which reflects their inherit ordering. Consequently each catogorical feature will only be normalised to be between 0 and 1, the common range for all transformed numeric features. This will be achieved with the MinMaxScaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_categorical_df = train_set_df[['KitchenQual', 'OverallQual', 'GarageFinish']]\n",
        "test_set_categorical_df = test_set_df[['KitchenQual', 'OverallQual', 'GarageFinish']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_scaler.set_output(transform='pandas')\n",
        "min_max_scaler.fit(train_set_categorical_df)\n",
        "# transforming train set\n",
        "train_set_categorical_df = min_max_scaler.transform(train_set_categorical_df)\n",
        "train_set_df[train_set_categorical_df.columns] = train_set_categorical_df\n",
        "# transforming test set\n",
        "test_set_categorical_df = min_max_scaler.transform(test_set_categorical_df)\n",
        "test_set_df[test_set_categorical_df.columns] = test_set_categorical_df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target scaling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some algorithms require the target to be normalised for optimal performance, consequently the target will be normalised (range 0-1) using the MinMaxScaler."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train set target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig , axes = plt.subplots(ncols=2)\n",
        "# before scaling histogram\n",
        "sns.histplot(data=train_set_df['SalePrice'], ax=axes[0])\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_scaler.set_output(transform='pandas')\n",
        "min_max_scaler.fit(train_set_df[['SalePrice']])\n",
        "transformed_df = min_max_scaler.transform(train_set_df[['SalePrice']])\n",
        "train_set_df[['SalePrice']] = transformed_df\n",
        "# histogram after scaling\n",
        "sns.histplot(data=train_set_df['SalePrice'], ax=axes[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "test set target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig , axes = plt.subplots(ncols=2)\n",
        "# before scaling histogram\n",
        "sns.histplot(data=test_set_df['SalePrice'], ax=axes[0])\n",
        "\n",
        "transformed_df = min_max_scaler.transform(test_set_df[['SalePrice']])\n",
        "test_set_df[['SalePrice']] = transformed_df\n",
        "# histogram after scaling\n",
        "sns.histplot(data=test_set_df['SalePrice'], ax=axes[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further feature selection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having now finished feature scaling, a further feature selection will be performed using a SGD Regressor model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating the SGD regression coefficients "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "sgd_regressor = SGDRegressor(random_state=30)\n",
        "sgd_regressor.fit(train_set_df.drop('SalePrice', axis=1), train_set_df['SalePrice'])\n",
        "print('R^2 score:', sgd_regressor.score(train_set_df.drop('SalePrice', axis=1), train_set_df['SalePrice']))\n",
        "print('\\n', 'coefficients')\n",
        "pd.DataFrame(data=sgd_regressor.coef_, index=train_set_df.drop('SalePrice', axis=1).columns).sort_values(by=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examining the coefficients reveals that the 'EnclosedPorchSF' feature coefficient is an order of magnitude less than all other coefficients. As such this feature will\n",
        "be dropped. There are other features that have smaller coefficients that could also be possibly dropped, but they are relatively large to warrant caution with regard to dropping: they may have greater importance in other more optimised models. \n",
        "\n",
        "Also removing too many features may risk under-fitting. If the end model constructed using these features fails to meet the necessary performance metrics, say due to over-fitting, then dropping more features may be an option to improve performance."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transforming the train set and test set by dropping the EnclosedPorchSF feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "drop_feature = DropFeatures('EnclosedPorchSF')\n",
        "drop_feature.fit(train_set_df)\n",
        "# transforming train set\n",
        "train_set_df = drop_feature.transform(train_set_df)\n",
        "# transforming test set\n",
        "test_set_df = drop_feature.transform(test_set_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train_set_df.head())\n",
        "test_set_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating custom transformers for pipelines"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the initial feature selection custom transformer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class CompositeSelectKBest(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom transformer that is a composition of two SelectKBest transformers.\n",
        "\n",
        "    Filters out a set union of best features, from a dataset with a SalePrice target.\n",
        "    \"\"\"\n",
        "    def fit(self, x, y):\n",
        "        \"\"\"\n",
        "        Composed of two SelectKBest fittings with spearman and phi_k score functions\n",
        "        \"\"\"\n",
        "        self.x_column_names = x.columns.to_list()\n",
        "\n",
        "        def spearman_score_function(feature_array, target_array):\n",
        "            \"\"\"\n",
        "            Calculates spearman correlation coefficients for x's features and y's target.\n",
        "\n",
        "            To be used as the score function in the sklearn.feature_selection.SelectKBest function.\n",
        "\n",
        "            Args:\n",
        "                feature_array = array-like, containing x data.\n",
        "                target_array = array-like, containing y data.\n",
        "\n",
        "            Returns tuple of the spearman r values series, and the spearman p values series for the dataset.\n",
        "            \"\"\"\n",
        "            df = pd.DataFrame(data=feature_array, columns=self.x_column_names)\n",
        "            df['SalePrice'] = target_array\n",
        "            spearman_df = df.pairwise_corr(columns=['SalePrice'], method='spearman')\n",
        "            return (spearman_df['r'], spearman_df['p-unc'])\n",
        "\n",
        "        self.spearman_score = spearman_score_function\n",
        "\n",
        "        def phik_score_function(feature_array, target_array):\n",
        "            \"\"\"\n",
        "            Calculates phi_k correlation coefficients for x's features and y's target.\n",
        "\n",
        "            To be used as the score function in the sklearn.feature_selection.SelectKBest function.\n",
        "\n",
        "            Args:\n",
        "                feature_array = array-like, containing x data.\n",
        "                target_array = array-like, containing y data\n",
        "\n",
        "            Returns tuple of the phi_k correlation values series, and the significance values series for the dataset.\n",
        "            \"\"\"\n",
        "            df = pd.DataFrame(data=feature_array, columns=self.x_column_names)\n",
        "            df['SalePrice'] = target_array\n",
        "        \n",
        "            phik_matrix_df = phik_matrix(df)[['SalePrice']].drop('SalePrice', axis=0)\n",
        "            significance_matrix_df = significance_matrix(df)[['SalePrice']].drop('SalePrice', axis=0)\n",
        "            return (phik_matrix_df['SalePrice'], significance_matrix_df['SalePrice'])\n",
        "\n",
        "        self.phik_score = phik_score_function\n",
        "\n",
        "        select_k_best_spear = SelectKBest(score_func=self.spearman_score)\n",
        "        select_k_best_spear.set_output(transform='pandas')\n",
        "        self.spear_trans = select_k_best_spear.fit(x, y)\n",
        "\n",
        "        select_k_best_phi = SelectKBest(score_func=self.phik_score)\n",
        "        select_k_best_phi.set_output(transform='pandas')\n",
        "        self.phik_trans = select_k_best_phi.fit(x, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        \"\"\"\n",
        "        Filters out the union of each set of the 10 best features obtained using the SelectKBest method with a phi_k and spearman score function.\n",
        "        \"\"\"\n",
        "        spearman_transformed_df = self.spear_trans.transform(x)\n",
        "        phik_transformed_df = self.phik_trans.transform(x)\n",
        "\n",
        "        selected_features = list(set(spearman_transformed_df.columns.to_list() + phik_transformed_df.columns.to_list()))\n",
        "        x = x[selected_features]\n",
        "        return x  \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating custom transformer for feature scaling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Need to use Normalizer for numeric features, but MinMaxScaler for categorical features. Will combine the effects of each transformer acting on the correct subset of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class CompositeNormaliser(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom transformer that collectively applies a Normalizer transformer to numeric features and applies a MinMaxScalar to categorical features.\n",
        "    \"\"\"\n",
        "    def fit(self, x, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        self.numeric_df = x.drop(['OverallQual', 'KitchenQual', 'GarageFinish'], axis=1)\n",
        "        self.categorical_df = x[['OverallQual', 'KitchenQual', 'GarageFinish']]\n",
        "        # transforming numeric features\n",
        "        normalizer = Normalizer()\n",
        "        normalizer.set_output(transform='pandas')\n",
        "        self.numeric_df = normalizer.fit_transform(self.numeric_df)\n",
        "        x[self.numeric_df.columns] = self.numeric_df\n",
        "        # transformng categorical features\n",
        "        min_max_scaler = MinMaxScaler()\n",
        "        min_max_scaler.set_output(transform='pandas')\n",
        "        self.categorical_df = min_max_scaler.fit_transform(self.categorical_df)\n",
        "        x[self.categorical_df.columns] = self.categorical_df\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data cleaning and feature engineering pipeline steps**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Will use an OrdinalEncoder transformer to encode the categorical variables: ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'], using the orderings:  \n",
        "bsmt_fin_type1_cat = np.array(['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'])  \n",
        "bsmt_exposure_cat = np.array(['None', 'No', 'Mn', 'Av', 'Gd'])  \n",
        "garage_finish_cat = np.array(['None', 'Unf', 'RFn', 'Fin'])  \n",
        "kitchen_quality_cat = np.array(['Po', 'Fa', 'TA', 'Gd', 'Ex'])  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Modelling pipeline steps**\n",
        "\n",
        "* For an initial feature selection will use a custom transformer CompositeSelectKBest with all features.\n",
        "* Will then remove redundant features using a SmartCorrelatedSelection transformer with decision tree estimator.\n",
        "* Will then scale all remaining numeric features using a Normalizer transformer.\n",
        "* Will also scale all remaining categorical features using a MinMaxScaler.\n",
        "* Will also scale the target using a MinMaxScaler.\n",
        "* Will use a DropFeatures transformer to drop the feature 'EnclosedPorchSF'.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
