{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "**Perform Business requirement 2 user story task: Feature engineering ML tasks**\n",
        "* Perform categorical encoding on categorical features.\n",
        "* Perform feature selection to distill the most significant features, and also remove redundant features.\n",
        "* Carry out feature scaling/transformations to normalise the distributions of remaining features.\n",
        "* Create the data cleaning and feature engineering pipeline.\n",
        "\n",
        "## Inputs\n",
        "* cleaned train set: outputs/datasets/ml/cleaned/train_set.csv\n",
        "* cleaned test set: outputs/datasets/ml/cleaned/test_set.csv\n",
        "\n",
        "## Outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "Working directory changed to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load cleaned train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_set_df = pd.read_csv(filepath_or_buffer='outputs/datasets/ml/cleaned/train_set.csv')\n",
        "test_set_df = pd.read_csv(filepath_or_buffer='outputs/datasets/ml/cleaned/test_set.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical feature encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the sale price correlation study notebook, the categorical features were encoded using an ordinal encoder; this was deemed most suitable since all the cateogrical features are ordinal, with an obvious ordering based around a rating.\n",
        "\n",
        "The exact same encoding will be used for the cleaned train and test sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_categorical_df = train_set_df.select_dtypes(include='object')\n",
        "print(train_set_categorical_df.columns)\n",
        "test_set_categorical_df = test_set_df.select_dtypes(include='object')\n",
        "test_set_categorical_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Designating the ordered categories\n",
        "bsmt_fin_type1_cat = np.array(list(reversed(['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'])))\n",
        "bsmt_exposure_cat = np.array(['None', 'No', 'Mn', 'Av', 'Gd'])\n",
        "garage_finish_cat = np.array(['None', 'Unf', 'RFn', 'Fin'])\n",
        "kitchen_quality_cat = np.array(['Po', 'Fa', 'TA', 'Gd', 'Ex'])\n",
        "\n",
        "categories = [bsmt_exposure_cat, bsmt_fin_type1_cat, garage_finish_cat, kitchen_quality_cat]\n",
        "encoder = OrdinalEncoder(categories=categories, dtype='int64')\n",
        "encoder.set_output(transform='pandas')\n",
        "\n",
        "# fitting and transforming each set\n",
        "train_set_df[train_set_categorical_df.columns] = encoder.fit_transform(X=train_set_categorical_df)\n",
        "print(train_set_df[train_set_categorical_df.columns].head())\n",
        "test_set_df[test_set_categorical_df.columns] = encoder.transform(X=test_set_categorical_df)\n",
        "test_set_df[test_set_categorical_df.columns].head()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial feature selection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It was decided to perform an initial feature selection before feature scaling, for primarily two reasons. Firstly there is quite a large number of features (23), for which performing feature scaling would be time consuming, as well as pointless for several features which have much less significance with regard to the target; this was already discovered in the sales price correlation study notebook, where a group of significant features was generated using correlation tests and PPS's applied to the whole dataset. The second reason is that as this initial selection will not use a ML model to establish the significant features, scaling is not important. \n",
        "\n",
        "Instead a similar approach to that used in the sales price correlation study involving correlation tests, but with the 'selectKBest method', will be used --- again this is not affected by scaling. \n",
        "\n",
        "After this is performed, feature-feature correlations will be assessed, and one of the features of any strongly correlated group will be dropped in order to reduce redundancy; which was seen to exist in the significant feature EDA notebook. A decision tree algorithm will be used, as this is unaffected by scaling.\n",
        "\n",
        "Feature scaling will then be performed on the most significant features, before a further feature selection will occur that relies on scale affected ML models. Since scaling has been performed, this should pose no issues."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**SelectKBest feature selection**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SelectKBest feature selection will be performed twice, once using the Spearman correlation test, then using the phi_k correlation test."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the score functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pingouin as pg\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "def spearman_score_function(feature_array, target_array):\n",
        "    \"\"\"\n",
        "    Calculates spearman correlation coefficients for the train or test set.\n",
        "\n",
        "    To be used as the score function in the sklearn.feature_selection.SelectKBest function.\n",
        "\n",
        "    Args:\n",
        "        feature_array = array-like, containing train or test set feature data.\n",
        "        target_array = array-like, containing train or test set target data\n",
        "\n",
        "    Returns tuple of the spearman r values series, and the spearman p values series for the dataset.\n",
        "    \"\"\"\n",
        "    column_names = train_set_df.columns.to_list()\n",
        "    df = pd.DataFrame(data=feature_array, columns=column_names[0:-1])\n",
        "    df['SalePrice'] = target_array\n",
        "    spearman_df = df.pairwise_corr(columns=['SalePrice'], method='spearman')\n",
        "    print('Spearman coefficients')\n",
        "    print(spearman_df.sort_values(by='r')[['X', 'Y', 'r', 'p-unc']])\n",
        "    return (spearman_df['r'], spearman_df['p-unc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import phik\n",
        "from phik.phik import phik_matrix\n",
        "from phik.report import plot_correlation_matrix\n",
        "from phik.significance import significance_matrix\n",
        "\n",
        "def phik_score_function(feature_array, target_array,):\n",
        "    \"\"\"\n",
        "    Calculates phi_k correlation coefficients for the train or test set.\n",
        "\n",
        "    To be used as the score function in the sklearn.feature_selection.SelectKBest function.\n",
        "\n",
        "    Args:\n",
        "        feature_array = array-like, containing train or test set feature data.\n",
        "        target_array = array-like, containing train or test set target data\n",
        "\n",
        "    Returns tuple of the phi_k correlation values series, and the significance values series for the dataset.\n",
        "    \"\"\"\n",
        "    column_names = train_set_df.columns.to_list()\n",
        "    df = pd.DataFrame(data=feature_array, columns=column_names[0:-1])\n",
        "    df['SalePrice'] = target_array\n",
        "   \n",
        "    phik_matrix_df = phik_matrix(df)[['SalePrice']].drop('SalePrice', axis=0)\n",
        "    significance_matrix_df = significance_matrix(df)[['SalePrice']].drop('SalePrice', axis=0)\n",
        "    print('phi_k coefficients')\n",
        "    print(phik_matrix_df)\n",
        "\n",
        "    return (phik_matrix_df['SalePrice'], significance_matrix_df['SalePrice'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will take the union of filtered out features using each score function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_out_best_features(df):\n",
        "    \"\"\"\n",
        "    Applies a SelectKBest transformer, once with a spearman score function, and then with a phi_k score function.\n",
        "\n",
        "    Args:\n",
        "        df: train or test set dataframe containing features and a target.\n",
        "\n",
        "    Returns a list of the union of the k-best-selected features generated using each score function.\n",
        "    \"\"\"\n",
        "    select_k_best = SelectKBest(score_func=spearman_score_function)\n",
        "    select_k_best.set_output(transform='pandas')\n",
        "    train_set_spearman_transformed_df = select_k_best.fit_transform(df.drop('SalePrice', axis=1), df['SalePrice'])\n",
        "\n",
        "    select_k_best = SelectKBest(score_func=phik_score_function)\n",
        "    select_k_best.set_output(transform='pandas')\n",
        "    train_set_phik_transformed_df = select_k_best.fit_transform(df.drop('SalePrice', axis=1), df['SalePrice'])\n",
        "\n",
        "    return list(set(train_set_spearman_transformed_df.columns.to_list() + train_set_phik_transformed_df.columns.to_list()))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features = filter_out_best_features(train_set_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Best features list**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of features:', len(selected_features))\n",
        "selected_features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can see that most of the selected features match those selected during the sale price correlation study notebook using the whole dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now will manually transform the train set by filtering out only the selected features. Will also filter out the same features from the test set, in order to avoid data leakage by\n",
        "selecting the best features separately using the test set instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_df = train_set_df[selected_features + ['SalePrice']]\n",
        "test_set_df = test_set_df[selected_features + ['SalePrice']]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling redundant features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will select a single feature from any highly correlated feature groups, using the Spearman correlation test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "smart_correlated_transformer = SmartCorrelatedSelection(train_set_df.drop('SalePrice', axis=1).columns.to_list(),\n",
        "                                                        method='spearman', threshold=0.8, selection_method='model_performance',\n",
        "                                                        estimator=estimator, scoring='r2', cv=5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "fitting to train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "smart_correlated_transformer.fit(X=train_set_df, y=train_set_df['SalePrice'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for statement in [smart_correlated_transformer.variables,\n",
        "                  smart_correlated_transformer.correlated_feature_sets_,\n",
        "                  smart_correlated_transformer.features_to_drop_]:\n",
        "                  print(statement)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "transforming train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set_df = smart_correlated_transformer.transform(train_set_df)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "transforming test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_df = smart_correlated_transformer.transform(test_set_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
