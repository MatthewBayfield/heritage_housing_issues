{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Modelling and Evaluation**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "**Perform Business requirement 2 user story tasks: model selection, pipeline creation, hyperparameter tuning, model evaluation.**\n",
        "* Create initial data cleaning and engineering pipeline using information from previous notebooks.\n",
        "* Create initial modelling and evaluation pipeline using information from previous notebooks.\n",
        "* Find best model candidate.\n",
        "* Optimise chosen model through tuning and feature selection using feature importance. \n",
        "* Evaluate the model performance using performance metrics.\n",
        "* Successfully achieve $R^2 \\ge 0.75$ for the final model, to satisfy the client's success criteria, and thereby satisfy business requirement 2.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "* house prices dataset: outputs/datasets/collection/house_prices.csv.\n",
        "* Information regarding the steps to include in the various pipelines, as indicated in the conclusion sections of the data cleaning and feature engineering notebooks.\n",
        "* Outlier indices list: outputs/ml/outlier_indices.pkl\n",
        "\n",
        "## Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "Working directory changed to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load house price dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "house_prices_df = pd.read_csv(filepath_or_buffer='outputs/datasets/collection/house_prices.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing known outliers from the whole dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading outlier indices list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "outlier_indices = joblib.load('outputs/ml/outlier_indices.pkl')\n",
        "outlier_indices"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing the instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_prices_df.drop(labels=outlier_indices, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create data cleaning and feature engineering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import src.ml.transformers_and_functions as tf\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
        "from feature_engine.selection import SmartCorrelatedSelection, DropFeatures\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_cleaning_and_feature_engineering():\n",
        "    \"\"\"\n",
        "    Constructs and returns data cleaning and feature engineering pipeline.\n",
        "    \"\"\"\n",
        "    # variables for defining pipeline\n",
        "    estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "    # Orginally intended to include the categories parameter used previously for OrdinalEncoder in the\n",
        "    # feature engineering notebook. However causes problems when not all category options are\n",
        "    # present in the train or test set. Will use the 'auto' option instead. \n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                        # Data cleaning:\n",
        "                        # Missing value imputation:\n",
        "                        ('IndependentKNNImputer', tf.IndependentKNNImputer()),\n",
        "                        ('EqualFrequencyImputer', tf.EqualFrequencyImputer()),\n",
        "                        #feature engineering:\n",
        "                        # encoding:\n",
        "                        ('OrdinalEncoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype='int64')),\n",
        "                        # feature number reduction\n",
        "                        ('CompositeSelectKBest', tf.CompositeSelectKBest()),\n",
        "                        ('SmartCorrelatedSelection', SmartCorrelatedSelection(method='spearman',\n",
        "                                                                              threshold=0.8, selection_method='model_performance',\n",
        "                                                                              estimator=estimator, scoring='r2', cv=5)),\n",
        "                        # #feature scaling:\n",
        "                        ('CompositeNormaliser', tf.CompositeNormaliser())\n",
        "                        ])\n",
        "    return pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As commented inside the data cleaning and feature engineering function above, it was intended to manually specify the ordinal encoding mapping using ordered arrays, as was done in the feature engineering notebook. However the train and test sets might not have all feature options for all features. For example for the train set the feature 'KitchenQual' does not have the value 'Po'. Therefore the encoding will be done automatically, and consequently the natural ranking of the feature values for a feature may not be preserved. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_cleaning_and_feature_engineering_pipeline = data_cleaning_and_feature_engineering()\n",
        "data_cleaning_and_feature_engineering_pipeline.set_output(transform='pandas')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_set_df, test_set_df) = train_test_split(house_prices_df, test_size=0.25, random_state=30)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the train and test sets in to their features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = train_set_df.drop('SalePrice', axis=1)\n",
        "y_train = train_set_df['SalePrice']\n",
        "x_test = test_set_df.drop('SalePrice', axis=1)\n",
        "y_test = test_set_df['SalePrice']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Scale target function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scale_target(y_train, y_test=None):\n",
        "    \"\"\"\n",
        "    Scales target for the train and or test set.\n",
        "\n",
        "    Args:\n",
        "        y_train: train target values.\n",
        "        y_test: test target values.\n",
        "\n",
        "    Returns a tuple of the scaled train and or test target series, as well as the inverse transform.\n",
        "    \"\"\"\n",
        "    y_train = pd.DataFrame(data=y_train)\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.set_output(transform='pandas')\n",
        "    min_max_scaler.fit(y_train)\n",
        "    y_train = min_max_scaler.transform(y_train)\n",
        "    inverse_transform = min_max_scaler.inverse_transform\n",
        "\n",
        "\n",
        "    if y_test is not None:\n",
        "        y_test = pd.DataFrame(data=y_test)\n",
        "        y_test = min_max_scaler.transform(y_test)\n",
        "        return (y_train.iloc[:, 0], y_test.iloc[:, 0], inverse_transform)\n",
        "\n",
        "    return (y_train.iloc[:, 0], inverse_transform)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Grid Search CV"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initially a search will be done to find the most suitable algorithm using sklearn's 'GridSearchCV', using only the default hyperparameters for each algorithm.\n",
        "Hyperparmeter tuning will then be performed for this best candidate algorithm, again using 'GridSearchCV', but with multiple hyperparameter value combinations."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best algorithm search"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a search class to handle the searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# taken from code-Institute-Solutions/churnometer (https://github.com/Code-Institute-Solutions/churnometer)\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "            model = self.models[key]\n",
        "\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring)\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing parameters for conducting search."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a dictionary of candidate models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
        "\n",
        "models = {'LinearRegression': LinearRegression(),\n",
        "          'DecisionTreeRegressor': DecisionTreeRegressor(random_state=30),\n",
        "          'RandomForestRegressor': RandomForestRegressor(random_state=30),\n",
        "          'ExtraTreeRegressor': ExtraTreeRegressor(random_state=30),\n",
        "          'AdaBoostRegressor': AdaBoostRegressor(random_state=30),\n",
        "          'BaggingRegressor': BaggingRegressor(random_state=30)}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the model parameters for each model; in this case there are no specified parameters meaning the default parameters will be used only as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_model_params = {'LinearRegression': {},\n",
        "                        'DecisionTreeRegressor': {},\n",
        "                        'RandomForestRegressor': {},\n",
        "                        'ExtraTreeRegressor': {},\n",
        "                        'AdaBoostRegressor': {},\n",
        "                        'BaggingRegressor': {}}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applying the data cleaning and engineering pipeline to a copy of the train set features, and scaling a copy of the train target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = x_train.copy(deep=True)\n",
        "y_train_copy = y_train.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_copy = scale_target(y_train.copy(deep=True))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = data_cleaning_and_feature_engineering_pipeline.fit(x_train_copy, y_train_copy).transform(x_train_copy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing the search with the created parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models, default_model_params)\n",
        "search.fit(x_train_copy, y_train_copy, scoring='r2', cv=5, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary, grid_search_pipelines = search.score_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All estimators have small or fairly small (<0.1*mean) standard deviations. The best max and mean score was achieved by the 'RandomForestRegressor', closely followed by the 'BaggingRegressor'. The top three estimators all achieved min scores better than the desired minimum of $R^2=0.75$.\n",
        "\n",
        "All things considered the 'RandomForestRegressor' seems to be the best model candidate. Hyperparameter tuning will now be performed for this model using GridSearchCV with multiple\n",
        "hyperparameter combinations, aided by the use of the HyperparameterOptimizationSearch class.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chosen best model candidate hyperparameter tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "updating models parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {'RandomForestRegressor': RandomForestRegressor(random_state=30)}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choosing the model hyperparameter combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 7 hyperparamters that will be tuned:\n",
        "\n",
        "* max_depth\n",
        "* max_leaf_nodes\n",
        "* min_samples_split\n",
        "* min_samples_leaf\n",
        "* n_estimators\n",
        "* max_features\n",
        "* max_samples"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ultimate goal is to avoid under-fitting and over-fitting the train set, leading to either high bias and low variance or low bias but high variance.\n",
        "The other factor to consider is computation cost/time and complexity, with more complex models or higher values of hyperparameters, such as for max_depth, taking a lot more time to compute for the same computation power.\n",
        "\n",
        "Ultimately there is a trade-off that must be decided subject to constraints and the success metric criteria.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of the 7 parameters selected for tuning, some will counteract/limit the effects of each other for certain values. For example increasing max depth, everything else held constant, would lead to more nodes/layers in the tree, provided the min_samples_split/max_leaf_nodes/min_samples_split are not limiting more nodes/layers being formed.\n",
        "\n",
        "What's more many of the parameters possess threshold values, either side of which the model performance on either or the train and test increases, decreases or plateaus.\n",
        "Time permitting ideally a large range (small step-size) of parameters would be trialed and validation curves, for a single or group of parameters, plotted to discover the optimum values."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this project, 3 values will be selected for each parameter and the impact of the various combinations assessed through using GridSearchCV via the HyperparameterOptimizationSearch class."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**max_depth**: the maximum number of layers\n",
        "\n",
        "* As a starting point, the number of features will be used, corresponding to a possible tree structure where each feature is used once (assuming max_features matches) to\n",
        "split a node. Will then take a value, half this, twice this, and two values in between. Hopefully this will indicate the rough location of a threshold value.\n",
        "* So values [5,10,20].\n",
        "\n",
        "**max_leaf_nodes**: the maximum number of terminal nodes. This will influence the number of nodes that can be split.\n",
        "\n",
        "* If every node in every layer is split the number of nodes (n) increases with the depth at a rate $2^{n-1}$. If every node is split ending in leaves for a given depth n,\n",
        "then the number of leaves will be $2^{n}$.\n",
        "* The higher the number of leaf nodes, the more nodes that can be split, the greater the complexity/computation. For n=10 the described structure would have 512 leaves. Also more leaves may lead to over-fitting.\n",
        "* Will cap the leaves at ~25% of this value\n",
        "* So considering the depth values chosen, will take values [32,130,250].\n",
        "\n",
        "**min_samples_split**: minimum number of samples at a node to split. This will counteract the number of leaves and the tree depth.\n",
        "* 1460 samples. Taking the scenario where on average each split equally divides the samples, then it would take $n=log(1460)/log(2)$ levels to give pure leaves, if every node is split. So roughly 11 levels. Again all pure leaves likely leads to overfitting.\n",
        "* So in this scenario lets say leaves have 10% max of the 1460 samples, giving a min_sample_split of 8. Will use this as a starting point.\n",
        "* Will choose values [4,8,64].\n",
        "\n",
        "**min_samples_leaf**: the minimum samples needed for a node to be a leaf.\n",
        "* At worst would want a leaf to have no more than 5% of the samples, and probably not 1 sample either.\n",
        "* Will use values [5,35,65].\n",
        "\n",
        "**n_estimators**: number of trees in the forest. The more the better up to a point where the performance plateaus. However more trees equals greater computation time.\n",
        "* The default value is 100, so will use this as a guide.\n",
        "* Will use values [50,150,250].\n",
        "\n",
        "**max_features**: The max number of features in a random subset to be used in a tree. Again this increases with the number of features up to a point but tails off and decreases.\n",
        "* Apparently a good value for this can be obtained using the sqrt(no. of features).\n",
        "* Also according to the sklearn documentation values close to 100% of the features give good empirical results.\n",
        "* will use values ['sqrt',0.66,1.0].\n",
        "\n",
        "**max_samples**: The sample size of the subset of samples.\n",
        " * Apparently a larger sample size increases the performance, but saturates quickly, and that only a small fraction of the sample is needed generally to achieve this saturation.\n",
        " * Thus will try the values [0.15,0.33,0.5].\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Setting the model parameters using the chosen values.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_params = {'RandomForestRegressor': {\n",
        "    'max_depth': [5,10,20],\n",
        "    'max_samples': [0.15,0.33,0.5],\n",
        "    'max_features': ['sqrt',0.66,1.0],\n",
        "    'n_estimators': [50,150,250],\n",
        "    'min_samples_leaf': [5,35,65],\n",
        "    'min_samples_split': [4,8,64],\n",
        "    'max_leaf_nodes': [32,130,250]\n",
        "}}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Originally attempted to use 5 values for each parameter, but the computation time was far too long."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Performing the search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models, model_params)\n",
        "search.fit(x_train_copy, y_train_copy, scoring='r2', cv=5, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary, grid_search_pipelines = search.score_summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Displaying the top 10 estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best estimator has a slightly worse mean/max/min score for the train set relative to the default parameters. However this may not be a bad thing, since a model that fits\n",
        "the train set too well, so low bias, may have higher variance."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Retrieving the best hyperparameter combination.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_results_summary.iloc[0, 0]\n",
        "best_model_params = grid_search_pipelines[best_model].best_params_\n",
        "print('Best model:', best_model)\n",
        "print('Best hyperparameter combination:', best_model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_regressor = grid_search_pipelines[best_model].best_estimator_\n",
        "best_regressor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Extracting the feature importances**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_features_out = data_cleaning_and_feature_engineering_pipeline['SmartCorrelatedSelection'].get_feature_names_out()\n",
        "regressor_feature_importances = best_regressor.feature_importances_\n",
        "feature_importances_df = pd.DataFrame(data=regressor_feature_importances, index=pipeline_features_out, columns=['importance']).sort_values(by='importance', ascending=False)\n",
        "feature_importances_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(tight_layout=True, figsize=(13,5))\n",
        "ax.set_title('Feature Importance')\n",
        "sns.barplot(data=feature_importances_df, x=feature_importances_df.index, y=feature_importances_df['importance'], ax=ax)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can see that half of the features are much less important than rest, and that the feature 'OverallQual' strongly dominates."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create model pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_pipeline = Pipeline([\n",
        "    ('RandomForestRegressor', best_regressor)\n",
        "])\n",
        "\n",
        "model_pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating model on train and test sets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating functions to evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "def model_evaluation(x_train, y_train, x_test, y_test, pipelines):\n",
        "    \"\"\"\n",
        "    Calculates predicted values for train and test sets. Prints statistics and plots assessing prediction accuracy.\n",
        "\n",
        "    Args:\n",
        "        x_train: train set feature data.\n",
        "        z_test: test set feature data.\n",
        "        y_train: actual train target values.\n",
        "        y_test: actual test target values.\n",
        "        pipelines: dictionary containing the data cleaning and engineering pipeline and the model pipeline.\n",
        "    \n",
        "    Returns tuple of the test and train predicted values, and the scaling inverse transform.\n",
        "    \"\"\"\n",
        "    # transform test and train set features\n",
        "    x_train = pipelines['data_cleaning_and_feature_engineering'].fit_transform(x_train, y_train)\n",
        "    x_test = pipelines['data_cleaning_and_feature_engineering'].transform(x_test)\n",
        "\n",
        "    # transform train and test set target\n",
        "    y_train, y_test, inverse_transform = scale_target(y_train, y_test)\n",
        "\n",
        "    # get target predictions\n",
        "    predictions_train = pipelines['model'].fit(x_train, y_train).predict(x_train)\n",
        "    predictions_test = pipelines['model'].predict(x_test)\n",
        "    predictions = (predictions_train, predictions_test)\n",
        "\n",
        "    # unscaling predictions\n",
        "    y_train = pd.DataFrame(inverse_transform(pd.DataFrame(data=y_train)))\n",
        "    y_test = pd.DataFrame(data=inverse_transform(pd.DataFrame(data=y_test)))\n",
        "    predictions_train = pd.DataFrame(inverse_transform(pd.DataFrame(data=predictions[0])))\n",
        "    predictions_test = pd.DataFrame(inverse_transform(pd.DataFrame(data=predictions[1])))\n",
        "\n",
        "    predictions = (predictions_train, predictions_test)\n",
        "\n",
        "\n",
        "    # # print summary performance statistics\n",
        "    model_evaluation_statistics(y_train, predictions_train)\n",
        "    model_evaluation_statistics(y_test, predictions_test)\n",
        "\n",
        "    # # print prediction vs actual plots\n",
        "    model_evaluation_plots(y_train, y_test, predictions)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "def model_evaluation_statistics(y, prediction):\n",
        "    \"\"\"\n",
        "    Prints statistics assessing prediction accuracy.\n",
        "\n",
        "    Args:\n",
        "        y: actual values array-likel\n",
        "        prediction: predicted values array-like.\n",
        "    \"\"\"\n",
        "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
        "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
        "    print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))\n",
        "    print('Root Mean Squared Error:', np.sqrt(\n",
        "        mean_squared_error(y, prediction)).round(3))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def model_evaluation_plots(y_train, y_test, predictions, alpha_scatter=0.5):\n",
        "    \"\"\"\n",
        "    Plots scatterplots including a line of perfect fit, for test and train actual values vs predicted values.\n",
        "\n",
        "    Args:\n",
        "        y_train: actual train target values.\n",
        "        y_test: actual test target values.\n",
        "        predictions: tuple of (predicted-train-target, predicted-test-target).\n",
        "    \"\"\"\n",
        "    # plotting scatterplots with a perfect fit line\n",
        "    prediction_train = predictions[0]\n",
        "    prediction_test = predictions[1]\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 8), tight_layout=True)\n",
        "    sns.scatterplot(x=y_train.iloc[:, 0], y=prediction_train.iloc[:, 0], alpha=alpha_scatter, ax=axes[0])\n",
        "    sns.lineplot(x=y_train.iloc[:, 0], y=y_train.iloc[:, 0], color='red', ax=axes[0])\n",
        "    axes[0].set_xlabel(\"Actual\")\n",
        "    axes[0].set_ylabel(\"Predictions\")\n",
        "    axes[0].set_title(\"Train Set\")\n",
        "\n",
        "    sns.scatterplot(x=y_test.iloc[:, 0], y=prediction_test.iloc[:, 0], alpha=alpha_scatter, ax=axes[1])\n",
        "    sns.lineplot(x=y_test.iloc[:, 0], y=y_test.iloc[:, 0], color='red', ax=axes[1])\n",
        "    axes[1].set_xlabel(\"Actual\")\n",
        "    axes[1].set_ylabel(\"Predictions\")\n",
        "    axes[1].set_title(\"Test Set\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluating the model on the train and test sets**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model_evaluation(x_train, y_train, x_test, y_test, {'data_cleaning_and_feature_engineering': data_cleaning_and_feature_engineering_pipeline,\n",
        "                                                                  'model': model_pipeline})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can see from the scatter plots and the result statistics for both the train and test set, that the model is over-fitted: the $R^2$ value for the test set is less than half of that of the train set. Additionally from the plot you can that the model consistently over predicts the sale price for the test set, but matches closely the train set. Overall for the current model, the test $R^2<0.75$, and so the success criterion is not met."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Refitting the model with less features**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly the model is biased towards the train set, and has high variance for this test set. Will try to refit the model with fewer features with the hope of reducing the likelihood of over-fitting to the train set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selecting the four features with the highest feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_four_features = feature_importances_df.sort_values(by='importance', ascending=False).iloc[0:4,:].index.values.tolist()\n",
        "features_to_drop = x_train.drop(best_four_features, axis=1).columns.tolist()\n",
        "print('Best four features:', best_four_features)\n",
        "print('features to drop:', features_to_drop)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "edit data cleaning and feature engineering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "import src.ml.transformers_and_functions as tf\n",
        "\n",
        "\n",
        "def data_cleaning_and_feature_engineering_refined():\n",
        "    \"\"\"\n",
        "    Constructs and returns data cleaning and feature engineering pipeline.\n",
        "    \"\"\"\n",
        "    # variables for defining pipeline\n",
        "    estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "    # Orginally intended to include the categories parameter used previously for OrdinalEncoder in the\n",
        "    # feature engineering notebook. However causes problems when not all category options are\n",
        "    # present in the train or test set. Will use the 'auto' option instead. \n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                        # Data cleaning:\n",
        "                        # Missing value imputation:\n",
        "                        ('IndependentKNNImputer', tf.IndependentKNNImputer()),\n",
        "                        ('EqualFrequencyImputer', tf.EqualFrequencyImputer()),\n",
        "                        #feature engineering:\n",
        "                        # encoding:\n",
        "                        ('OrdinalEncoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype='int64')),\n",
        "                        # feature number reduction\n",
        "                        ('DropFeatures', DropFeatures(features_to_drop=features_to_drop)),\n",
        "                        # feature scaling:\n",
        "                        ('CompositeNormaliser', tf.CompositeNormaliser())\n",
        "                        ])\n",
        "    return pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting the modified data cleaning and engineering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_cleaning_and_feature_engineering_pipeline_refined = data_cleaning_and_feature_engineering_refined()\n",
        "data_cleaning_and_feature_engineering_pipeline_refined.set_output(transform='pandas')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "creating copies of the train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = x_train.copy(deep=True)\n",
        "y_train_copy = y_train.copy(deep=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scaling the train target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_copy = scale_target(y_train.copy(deep=True))[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transforming the train set features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = data_cleaning_and_feature_engineering_pipeline_refined.fit(x_train_copy, y_train_copy).transform(x_train_copy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redo tuning with best four features only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_new = HyperparameterOptimizationSearch(models, model_params)\n",
        "search_new.fit(x_train_copy, y_train_copy, scoring='r2', cv=5, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary_best_four, grid_search_pipelines_best_four = search_new.score_summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary_best_four.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compared to the last search, the scores are lower for this smaller group of more important features. This however is desired to avoid over-fitting and bias towards the train set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Retrieving the best hyperparameter combination.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_results_summary_best_four.iloc[0, 0]\n",
        "best_model_params = grid_search_pipelines_best_four[best_model].best_params_\n",
        "print('Best model:', best_model)\n",
        "print('Best hyperparameter combination:', best_model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_regressor_best_four = grid_search_pipelines_best_four[best_model].best_estimator_\n",
        "best_regressor_best_four"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Extracting the feature importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_features_out = data_cleaning_and_feature_engineering_pipeline_refined['DropFeatures'].get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regressor_feature_importances_best_four = best_regressor_best_four.feature_importances_\n",
        "feature_importances_best_four_df = pd.DataFrame(data=regressor_feature_importances_best_four,\n",
        "                                                index=pipeline_features_out, columns=['importance']).sort_values(by='importance', ascending=False)\n",
        "feature_importances_best_four_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(tight_layout=True, figsize=(13,5))\n",
        "ax.set_title('Feature Importance')\n",
        "sns.barplot(data=feature_importances_best_four_df, x=feature_importances_best_four_df.index, y=feature_importances_best_four_df['importance'], ax=ax)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 'OverallQual' feature now dominates even more than before, with the remaining features having equal importance, and collectively having a combined importance value\n",
        "of about ~40% of the feature importance of 'OverallQual'. Does seem to suggest that 'OverallQual' almost single-handedly determines the sale price prediction. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Updating the model pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_pipeline = Pipeline([\n",
        "    ('RandomForestRegressor', best_regressor_best_four)\n",
        "])\n",
        "\n",
        "model_pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating model on the train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model_evaluation(x_train, y_train, x_test, y_test, {'data_cleaning_and_feature_engineering': data_cleaning_and_feature_engineering_pipeline_refined,\n",
        "                                                                  'model': model_pipeline})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen from the plots and the $R^2$ values that the model has significantly improved in predicting the test set target values, going from $R^2\\approx0.4$ to $R^2$=0.72.\n",
        "At the same the train set performance has declined a little to $R^2=0.76$, but as mentioned before this is a consequence of reducing the degree of over-fitting. It seems when fitting with more features, the feature importance of the feature 'OverallQual' declines a little and this leads to a poorer performance on the test set.\n",
        "\n",
        "Despite the improvement in performance on the test, the model performance is marginally below the success criterion of $R^2=0.75$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Possible next steps**:\n",
        "* Try an even smaller group of features, or a different combination of the most important features.\n",
        "* Perform hyperparameter tuning with a new range of hyperparamter values.\n",
        "* Make adjustments to the data cleaning and feature engineering pipeline.\n",
        "* Assess the similarity of the test and train set distributions to ensure a bad split, meaning one of the sets is not representative of the parent distribution, has not occurred. Can also try different random_state parameter values for splitting the dataset to test this."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refitting with 3 most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_three_features = feature_importances_df.sort_values(by='importance', ascending=False).iloc[0:3,:].index.values.tolist()\n",
        "features_to_drop = x_train.drop(best_three_features, axis=1).columns.tolist()\n",
        "print('Best three features:', best_three_features)\n",
        "print('features to drop:', features_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "import src.ml.transformers_and_functions as tf\n",
        "\n",
        "\n",
        "def data_cleaning_and_feature_engineering_refined():\n",
        "    \"\"\"\n",
        "    Constructs and returns data cleaning and feature engineering pipeline.\n",
        "    \"\"\"\n",
        "    # variables for defining pipeline\n",
        "    estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "    # Orginally intended to include the categories parameter used previously for OrdinalEncoder in the\n",
        "    # feature engineering notebook. However causes problems when not all category options are\n",
        "    # present in the train or test set. Will use the 'auto' option instead. \n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                        # Data cleaning:\n",
        "                        # Missing value imputation:\n",
        "                        ('IndependentKNNImputer', tf.IndependentKNNImputer()),\n",
        "                        ('EqualFrequencyImputer', tf.EqualFrequencyImputer()),\n",
        "                        #feature engineering:\n",
        "                        # encoding:\n",
        "                        ('OrdinalEncoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype='int64')),\n",
        "                        # feature number reduction\n",
        "                        ('DropFeatures', DropFeatures(features_to_drop=features_to_drop)),\n",
        "                        # feature scaling:\n",
        "                        ('CompositeNormaliser', tf.CompositeNormaliser())\n",
        "                        ])\n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_cleaning_and_feature_engineering_pipeline_refined = data_cleaning_and_feature_engineering_refined()\n",
        "data_cleaning_and_feature_engineering_pipeline_refined.set_output(transform='pandas')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = x_train.copy(deep=True)\n",
        "y_train_copy = y_train.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_copy = scale_target(y_train.copy(deep=True))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = data_cleaning_and_feature_engineering_pipeline_refined.fit(x_train_copy, y_train_copy).transform(x_train_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_new = HyperparameterOptimizationSearch(models, model_params)\n",
        "search_new.fit(x_train_copy, y_train_copy, scoring='r2', cv=5, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary_best_three, grid_search_pipelines_best_three = search_new.score_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary_best_three.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_results_summary_best_three.iloc[0, 0]\n",
        "best_model_params = grid_search_pipelines_best_three[best_model].best_params_\n",
        "print('Best model:', best_model)\n",
        "print('Best hyperparameter combination:', best_model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_regressor_best_three = grid_search_pipelines_best_three[best_model].best_estimator_\n",
        "best_regressor_best_three"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_features_out = data_cleaning_and_feature_engineering_pipeline_refined['DropFeatures'].get_feature_names_out()\n",
        "regressor_feature_importances_best_three = best_regressor_best_three.feature_importances_\n",
        "feature_importances_best_three_df = pd.DataFrame(data=regressor_feature_importances_best_three,\n",
        "                                                index=pipeline_features_out, columns=['importance']).sort_values(by='importance', ascending=False)\n",
        "feature_importances_best_three_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(tight_layout=True, figsize=(13,5))\n",
        "ax.set_title('Feature Importance')\n",
        "sns.barplot(data=feature_importances_best_three_df, x=feature_importances_best_three_df.index, y=feature_importances_best_three_df['importance'], ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_pipeline = Pipeline([\n",
        "    ('RandomForestRegressor', best_regressor_best_three)\n",
        "])\n",
        "\n",
        "model_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating model on the train and test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model_evaluation(x_train, y_train, x_test, y_test, {'data_cleaning_and_feature_engineering': data_cleaning_and_feature_engineering_pipeline_refined,\n",
        "                                                                  'model': model_pipeline})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
