{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Modelling and Evaluation**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "**Perform Business requirement 2 user story tasks: model selection, pipeline creation, hyperparameter tuning, model evaluation.**\n",
        "* Create initial data cleaning and engineering pipeline using information from previous notebooks.\n",
        "* Create initial modelling and evaluation pipeline using information from previous notebooks.\n",
        "* Find best model candidate.\n",
        "* Optimise chosen model through tuning and feature selection using feature importance. \n",
        "* Evaluate the model performance using performance metrics.\n",
        "* Successfully achieve $R^2 \\ge 0.75$ for the final model, to satisfy the client's success criteria, and thereby satisfy business requirement 2.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "* house prices dataset: outputs/datasets/collection/house_prices.csv.\n",
        "* Information regarding the steps to include in the various pipelines, as indicated in the conclusion sections of the data cleaning and feature engineering notebooks.\n",
        "* Outlier indices list: src/ml/outlier_indices.pkl\n",
        "\n",
        "## Outputs\n",
        "* train set: src/ml/train_set_df.csv\n",
        "* test set: src/ml/test_set_df.csv\n",
        "* pickled data cleaning and engineering pipeline: src/ml/data_cleaning_and_feature_engineering_pipeline.pkl\n",
        "* pickled model pipeline: src/ml/model_pipeline.pkl\n",
        "* feature importances CSV: src/ml/feature_importances_df.csv\n",
        "* model performances CSV: src/ml/model_pipeline.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "Working directory changed to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load house price dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "house_prices_df = pd.read_csv(filepath_or_buffer='outputs/datasets/collection/house_prices.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing known outliers from the whole dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading outlier indices list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "outlier_indices = joblib.load('src/ml/outlier_indices.pkl')\n",
        "outlier_indices"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing the instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_prices_df.drop(labels=outlier_indices, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create data cleaning and feature engineering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import src.ml.transformers_and_functions as tf\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
        "from feature_engine.selection import SmartCorrelatedSelection, DropFeatures\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from feature_engine.transformation import PowerTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def data_cleaning_and_feature_engineering():\n",
        "    \"\"\"\n",
        "    Constructs and returns data cleaning and feature engineering pipeline.\n",
        "    \"\"\"\n",
        "    # variables for defining pipeline\n",
        "    estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "    # Orginally intended to include the categories parameter used previously for OrdinalEncoder in the\n",
        "    # feature engineering notebook. However causes problems when not all category options are\n",
        "    # present in the train or test set. Will use the 'auto' option instead.\n",
        "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype='int64')\n",
        "    encoder.set_output(transform='pandas')\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.set_output(transform='pandas')\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                        # Data cleaning:\n",
        "                        # Missing value imputation:\n",
        "                        ('IndependentKNNImputer', tf.IndependentKNNImputer()),\n",
        "                        ('EqualFrequencyImputer', tf.EqualFrequencyImputer()),\n",
        "                        #feature engineering:\n",
        "                        # encoding:\n",
        "                        ('OrdinalEncoder', ColumnTransformer(transformers=[('encoder', encoder, ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'])],\n",
        "                                                      remainder='passthrough', n_jobs=-1, verbose_feature_names_out=False)),\n",
        "                        # feature scaling:\n",
        "                        ('Scaling', ColumnTransformer(transformers=[('Power', PowerTransformer(), ['YearBuilt', 'GrLivArea', 'BsmtFinSF1'])],\n",
        "                                                    remainder=min_max_scaler, n_jobs=-1, verbose_feature_names_out=False)),\n",
        "                        # feature number reduction\n",
        "                        ('CompositeSelectKBest', tf.CompositeSelectKBest()),\n",
        "                        ('SmartCorrelatedSelection', SmartCorrelatedSelection(method='spearman',\n",
        "                                                                              threshold=0.8, selection_method='model_performance',\n",
        "                                                                              estimator=estimator, scoring='r2', cv=5))\n",
        "                        ])\n",
        "                        \n",
        "    pipeline.set_output(transform='pandas')\n",
        "    return pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As commented inside the data cleaning and feature engineering function above, it was intended to manually specify the ordinal encoding mapping using ordered arrays, as was done in the feature engineering notebook. However the train and test sets might not have all feature options for all features. For example for the train set the feature 'KitchenQual' does not have the value 'Po'. Therefore the encoding will be done automatically, and consequently the natural ranking of the feature values for a feature may not be preserved. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_cleaning_and_feature_engineering_pipeline = data_cleaning_and_feature_engineering()\n",
        "data_cleaning_and_feature_engineering_pipeline.set_output(transform='pandas')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When random splitting the dataset into train and test subsets, as determined by the value of random_state, the quality of split and therefore the model performance on the test set\n",
        "will vary, the degree of which may be impacted by the inherent variability of the final model. If the train and or test sets are not representative of the original parent dataset, then the model performance may be adversely affected. The assumption being that the original dataset, a sample itself, is representative of the fixed parent population if one exists.\n",
        "The sizes of the train and test sets also indirectly impact the likelihood that a random split is representative, namely the more data in each the more likely it is; thus the larger the parent set as a whole, the less likely the random split will affect model performance significantly.\n",
        "\n",
        "Stratification by using a dataset variable may help to ensure the train and test sets are more representative, however if there are many uncorrelated variables in the dataset, skewed splits may still occur."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To assess the quality of a random split, plots and test statistics (with or without significance values) can be used. As there are countably infinite possible random_state values, even though the number of distinguishable splits may be finite, it may not be practical to discover the best split; however at the very least the train and test sets should be representative of the original dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The other factor that may be impact an aspect of split quality is the relative sizes of the train and test sets. The larger the train set, the more likely the model will learn any patterns in the data, and thus be able to make better predictions. The larger the test set size the greater the confidence in the accuracy and reliability of the model performance metric scores. Thus naturally there is a trade-off between having an optimally large test set or train set, at least for small datasets. For sufficiently large datasets, it is possible to have both an optimally large train and test set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The house prices dataset size is fairly small (~1500), and the random split is likely to affect model performance. Stratification by the sale price target will hopefully ensure a similar distribution for it in the train and test sets, and thus also for other strongly correlated features. Will perform 100 splits and assess the quality of each, and pick the best. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Creating the sale price bins needed for stratification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sale_price_bin_series = pd.cut(x=house_prices_df['SalePrice'], bins=10)\n",
        "print(sale_price_bin_series)\n",
        "house_prices_df['sale price bin'] = sale_price_bin_series\n",
        "house_prices_df[['SalePrice', 'sale price bin']].head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performing the 100 different random splits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identifying the continuous and discrete dataset variables, and partitioning the dataset accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "continuous_variables = ['1stFlrSF',\n",
        "                        '2ndFlrSF',\n",
        "                        'BsmtFinSF1',\n",
        "                        'BsmtUnfSF',\n",
        "                        'EnclosedPorchSF',\n",
        "                        'GarageArea',\n",
        "                        'GrLivArea',\n",
        "                        'LotArea',\n",
        "                        'LotFrontage',\n",
        "                        'MasVnrArea',\n",
        "                        'OpenPorchSF',\n",
        "                        'TotalBsmtSF',\n",
        "                        'WoodDeckSF',\n",
        "                        'SalePrice']\n",
        "continuous_variables_df = house_prices_df[continuous_variables]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "discrete_variables_df = house_prices_df.drop(continuous_variables, axis=1)\n",
        "discrete_variables = discrete_variables_df.columns.values.tolist()\n",
        "discrete_variables.remove('sale price bin')\n",
        "discrete_variables"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The quality of each split will be assessed by testing how representative the train and test sets are of the whole dataset. For each test and train distribution resulting from a split, chi-squared significance tests will be used for discrete variables, and the two-sample Kolmogorov-Smirnov significance test for continuous variables. A significance level of 50% will be used to set a low bar for rejecting the null hypothesis that the distribution of a variable in the train/test set is the same as its distribution in the whole dataset.\n",
        "For each split, for each subset, the number of variables that have distributions that make the null hypothesis true, will be counted. In addition the product of the individual p-values for each variable for both train and test sets combined will be generated. Collectively these metrics will allow the split quality to be estimated and ranked. It is likely the ranking of splits is not 100% correct due to the way which the p-value product is calculated (for example it assumes independent probabilities). In addition the model performance is not necessarily higher or more reliable for higher ranked splits. However the number of variables for which the null hypothesis is true should indicate whether a given split produces subsets that are fully representative of the whole dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Re-binning discrete variables that have bin frequencies less than 10 as per the requirements of the chi square test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_to_be_rebinned = []\n",
        "for variable in discrete_variables:\n",
        "    if not (discrete_variables_df[variable].value_counts(dropna=False) > 10).all():\n",
        "        variables_to_be_rebinned.append(variable)\n",
        "variables_to_be_rebinned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "binned_discrete_vars_df = discrete_variables_df.copy(deep=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating new bins for the required variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(discrete_variables_df['BedroomAbvGr'].value_counts(dropna=False))\n",
        "print(pd.cut(discrete_variables_df['BedroomAbvGr'], bins=[0,1,2,3,4,8]).value_counts(dropna=False))\n",
        "binned_discrete_vars_df['BedroomAbvGr'] = pd.cut(discrete_variables_df['BedroomAbvGr'], bins=[0,1,2,3,4,8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(discrete_variables_df['GarageYrBlt'].value_counts(dropna=False))\n",
        "print(pd.qcut(discrete_variables_df['GarageYrBlt'], q=10).value_counts(dropna=False))\n",
        "binned_discrete_vars_df['GarageYrBlt'] = pd.qcut(discrete_variables_df['GarageYrBlt'], q=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(discrete_variables_df['YearBuilt'].value_counts(dropna=False))\n",
        "print(pd.qcut(discrete_variables_df['YearBuilt'], q=10).value_counts(dropna=False))\n",
        "binned_discrete_vars_df['YearBuilt'] = pd.qcut(discrete_variables_df['YearBuilt'], q=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(discrete_variables_df['YearRemodAdd'].value_counts(dropna=False))\n",
        "print(pd.cut(discrete_variables_df['YearRemodAdd'], bins=10).value_counts(dropna=False))\n",
        "binned_discrete_vars_df['YearRemodAdd'] = pd.cut(discrete_variables_df['YearRemodAdd'], bins=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(discrete_variables_df['OverallCond'].value_counts(dropna=False))\n",
        "print(pd.cut(discrete_variables_df['OverallCond'], bins=[0,3,4,5,6,7,8,9]).value_counts(dropna=False))\n",
        "binned_discrete_vars_df['OverallCond'] = pd.cut(discrete_variables_df['OverallCond'], bins=[0,3,4,5,6,7,8,9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(discrete_variables_df['OverallQual'].value_counts(dropna=False))\n",
        "print(pd.cut(discrete_variables_df['OverallQual'], bins=[0,3,4,5,6,7,8,9,10]).value_counts(dropna=False))\n",
        "binned_discrete_vars_df['OverallQual'] = pd.cut(discrete_variables_df['OverallQual'], bins=[0,3,4,5,6,7,8,9,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "binned_discrete_vars_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Creating a dataframe to facilitate the random split quality comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_array = [['number of distributions where the the null hypothesis is true', 'p value product'], ['parent-train', 'parent-test', 'total']]\n",
        "multi_index = pd.MultiIndex(index_array, [np.append(np.repeat([0], 3), 1), np.append(np.tile([0,1,2], 1), 2)])\n",
        "multi_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distribution_comparison_df = pd.DataFrame(columns=multi_index, dtype='float')\n",
        "distribution_comparison_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 0\n",
        "while seed < 100:\n",
        "    house_prices_copy_df = house_prices_df.copy(deep=True)\n",
        "    house_prices_copy_df[(house_prices_df.drop(continuous_variables, axis=1)).columns] = binned_discrete_vars_df\n",
        "    # performing the split for the current seed\n",
        "    (train_set_df, test_set_df) = train_test_split(house_prices_copy_df, test_size=0.25, random_state=seed, stratify=house_prices_copy_df['sale price bin'])\n",
        "    # dropping the sale price bin column from dataframes\n",
        "    house_prices_copy_df.drop('sale price bin', axis=1, inplace=True)\n",
        "    train_set_df.drop('sale price bin', axis=1, inplace=True)\n",
        "    test_set_df.drop('sale price bin', axis=1, inplace=True)\n",
        "    # partitioning the train and test sets by their continuous and discrete variables\n",
        "    train_continuous_var_df = train_set_df[continuous_variables]\n",
        "    train_discrete_var_df = train_set_df[discrete_variables]\n",
        "    test_continuous_var_df = test_set_df[continuous_variables]\n",
        "    test_discrete_var_df = test_set_df[discrete_variables]\n",
        "    # creating an initial zero row for this seed in the distribution comparison dataframe\n",
        "    distribution_comparison_df = pd.concat(objs=[distribution_comparison_df, pd.DataFrame(index=[seed], columns=multi_index, data=np.array([np.append(np.repeat(0, 3), 1)]))])\n",
        "\n",
        "    # calculating the chi-squared test statistics for the discrete variables for each dataset comparison\n",
        "    for column in discrete_variables:\n",
        "\n",
        "        # parent-test distribution comparisons:\n",
        "        \n",
        "        # value proportion of the current variable in the whole dataset\n",
        "        value_proportion_series = binned_discrete_vars_df[column].value_counts(dropna=False)/binned_discrete_vars_df.index.size\n",
        "        # expected values for the current variable\n",
        "        expected = (value_proportion_series*test_discrete_var_df.index.size)\n",
        "        # observed values for the current variable\n",
        "        observed = test_discrete_var_df[column].value_counts(dropna=False)\n",
        "        # calculating the chi-squared statistic p-value for the current variable\n",
        "        p_value = scipy.stats.chisquare(observed, expected)[1]\n",
        "        # Assess significance value and update distribution_comparison df\n",
        "        if p_value > 0.5:\n",
        "            distribution_comparison_df.iloc[seed, 1] += 1\n",
        "        distribution_comparison_df.iloc[seed, 3] *= p_value\n",
        "       \n",
        "        # parent-train distribution comparisons:\n",
        "\n",
        "        observed = train_discrete_var_df[column].value_counts(dropna=False)\n",
        "        expected = (value_proportion_series*train_discrete_var_df.index.size).values.tolist()\n",
        "        observed = train_discrete_var_df[column].value_counts(dropna=False).values.tolist()\n",
        "        p_value = scipy.stats.chisquare(observed, expected)[1]\n",
        "        if p_value > 0.5:\n",
        "            distribution_comparison_df.iloc[seed, 0] += 1\n",
        "        distribution_comparison_df.iloc[seed, 3] *= p_value\n",
        "        \n",
        "    # calculating the KS test statistics for the continuous variables for each dataset comparison\n",
        "    for column in continuous_variables:\n",
        "        # parent-train distribution comparisons:\n",
        "\n",
        "        # calculating the KS test statistic for the current variable\n",
        "        p_value = scipy.stats.ks_2samp(continuous_variables_df[column], train_continuous_var_df[column])[1]\n",
        "        # Assess significance value and update distribution_comparison df\n",
        "        if p_value > 0.5:\n",
        "            distribution_comparison_df.iloc[seed, 0] += 1\n",
        "        distribution_comparison_df.iloc[seed, 3] *= p_value\n",
        "        \n",
        "        \n",
        "        # parent-test distribution comparisons:\n",
        "\n",
        "        p_value = scipy.stats.ks_2samp(continuous_variables_df[column], test_continuous_var_df[column])[1]\n",
        "        if p_value > 0.5:\n",
        "            distribution_comparison_df.iloc[seed, 1] += 1\n",
        "        distribution_comparison_df.iloc[seed, 3] *= p_value\n",
        "\n",
        "            \n",
        "    seed += 1\n",
        "# Calculating the total number of distributions for which the null hypothesis is true\n",
        "distribution_comparison_df.iloc[:, 2] = distribution_comparison_df.iloc[:, 0] + distribution_comparison_df.iloc[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distribution_comparison_df.sort_values(by=[('number of distributions where the the null hypothesis is true', 'total'), ('p value product', 'total')],\n",
        "                                       ascending=[False, False]).head(50)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thus using the above distribution comparison dataframe as a guide, a random_state value of 68 will be used to produce a representative split**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(train_set_df, test_set_df) = train_test_split(house_prices_df, test_size=0.25, random_state=68, stratify=house_prices_df['sale price bin'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_prices_df.drop('sale price bin', axis=1, inplace=True)\n",
        "train_set_df.drop('sale price bin', axis=1, inplace=True)\n",
        "test_set_df.drop('sale price bin', axis=1, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the train and test sets in to their features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = train_set_df.drop('SalePrice', axis=1)\n",
        "y_train = train_set_df['SalePrice']\n",
        "x_test = test_set_df.drop('SalePrice', axis=1)\n",
        "y_test = test_set_df['SalePrice']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Scale target function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scale_target(y_fit, y_transform):\n",
        "    \"\"\"\n",
        "    Scales target for a subset, having been trained on another subset.\n",
        "\n",
        "    Args:\n",
        "        y_fit: target values used for fitting.\n",
        "        y_transform: target values transformed.\n",
        "\n",
        "    Returns a tuple of the scaled target series, as well as the inverse transform.\n",
        "    \"\"\"\n",
        "    y_fit = pd.DataFrame(data=y_fit)\n",
        "    y_transform = pd.DataFrame(data=y_transform)\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.set_output(transform='pandas')\n",
        "    min_max_scaler.fit(y_fit)\n",
        "    y_transform = min_max_scaler.transform(y_transform)\n",
        "    inverse_transform = min_max_scaler.inverse_transform\n",
        "\n",
        "    return (y_transform.iloc[:, 0], inverse_transform)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the y_train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "file_path = 'src/ml'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    y_train.to_csv(f\"{file_path}/y_train.csv\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating functions needed to evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "def model_evaluation(x_train, y_train, x_test, y_test, pipelines):\n",
        "    \"\"\"\n",
        "    Calculates predicted values for train and test sets. Prints statistics and plots assessing prediction accuracy.\n",
        "\n",
        "    Args:\n",
        "        x_train: train set feature data.\n",
        "        x_test: test set feature data.\n",
        "        y_train: actual train target values.\n",
        "        y_test: actual test target values.\n",
        "        pipelines: dictionary containing the data cleaning and engineering pipeline and the model pipeline.\n",
        "    \n",
        "    Returns a tuple of predictions, train statistics, and test statistics.\n",
        "    \"\"\"\n",
        "    # transform train and test set target\n",
        "    y_test = scale_target(y_train, y_test)[0]\n",
        "    y_train, inverse_transform = scale_target(y_train, y_train)\n",
        "\n",
        "    # transform test and train set features\n",
        "    x_train = pipelines['data_cleaning_and_feature_engineering'].fit_transform(x_train, y_train)\n",
        "    x_test = pipelines['data_cleaning_and_feature_engineering'].transform(x_test)\n",
        "    \n",
        "    # get target predictions\n",
        "    predictions_train = pipelines['model'].fit(x_train, y_train).predict(x_train)\n",
        "    predictions_test = pipelines['model'].predict(x_test)\n",
        "    predictions = (predictions_train, predictions_test)\n",
        "\n",
        "    # unscaling predictions\n",
        "    y_train = pd.DataFrame(inverse_transform(pd.DataFrame(data=y_train)))\n",
        "    y_test = pd.DataFrame(data=inverse_transform(pd.DataFrame(data=y_test)))\n",
        "    predictions_train = pd.DataFrame(inverse_transform(pd.DataFrame(data=predictions[0])))\n",
        "    predictions_test = pd.DataFrame(inverse_transform(pd.DataFrame(data=predictions[1])))\n",
        "\n",
        "    predictions = (predictions_train, predictions_test)\n",
        "\n",
        "\n",
        "    # gather summary performance statistics\n",
        "    train_stats = model_evaluation_statistics(y_train, predictions_train)\n",
        "    test_stats = model_evaluation_statistics(y_test, predictions_test)\n",
        "\n",
        "    # print prediction vs actual plots\n",
        "    model_evaluation_plots(y_train, y_test, predictions)\n",
        "    \n",
        "    return predictions, train_stats, test_stats\n",
        "\n",
        "def model_evaluation_statistics(y, prediction):\n",
        "    \"\"\"\n",
        "    Prints statistics assessing prediction accuracy.\n",
        "\n",
        "    Args:\n",
        "        y: actual values array-like\n",
        "        prediction: predicted values array-like.\n",
        "    \n",
        "    Returns a list of statistics.\n",
        "    \"\"\"\n",
        "    statistics = [r2_score(y, prediction).round(3), mean_absolute_error(y, prediction).round(3),\n",
        "                  mean_squared_error(y, prediction).round(3), np.sqrt(mean_squared_error(y, prediction)).round(3)]\n",
        "    print('R2 Score:', statistics[0])\n",
        "    print('Mean Absolute Error:', statistics[1])\n",
        "    print('Mean Squared Error:', statistics[2])\n",
        "    print('Root Mean Squared Error:', statistics[3])\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return statistics\n",
        "\n",
        "\n",
        "def model_evaluation_plots(y_train, y_test, predictions, alpha_scatter=0.5):\n",
        "    \"\"\"\n",
        "    Plots scatterplots including a line of perfect fit, for test and train actual values vs predicted values.\n",
        "\n",
        "    Args:\n",
        "        y_train: actual train target values.\n",
        "        y_test: actual test target values.\n",
        "        predictions: tuple of (predicted-train-target, predicted-test-target).\n",
        "    \"\"\"\n",
        "    # plotting scatterplots with a perfect fit line\n",
        "    prediction_train = predictions[0]\n",
        "    prediction_test = predictions[1]\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 8), tight_layout=True)\n",
        "    sns.scatterplot(x=y_train.iloc[:, 0], y=prediction_train.iloc[:, 0], alpha=alpha_scatter, ax=axes[0])\n",
        "    sns.lineplot(x=y_train.iloc[:, 0], y=y_train.iloc[:, 0], color='red', ax=axes[0])\n",
        "    axes[0].set_xlabel(\"Actual\")\n",
        "    axes[0].set_ylabel(\"Predictions\")\n",
        "    axes[0].set_title(\"Train Set\")\n",
        "\n",
        "    sns.scatterplot(x=y_test.iloc[:, 0], y=prediction_test.iloc[:, 0], alpha=alpha_scatter, ax=axes[1])\n",
        "    sns.lineplot(x=y_test.iloc[:, 0], y=y_test.iloc[:, 0], color='red', ax=axes[1])\n",
        "    axes[1].set_xlabel(\"Actual\")\n",
        "    axes[1].set_ylabel(\"Predictions\")\n",
        "    axes[1].set_title(\"Test Set\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Grid Search CV"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initially a search will be done to find the most suitable algorithm using sklearn's 'GridSearchCV', using only the default hyperparameters for each algorithm.\n",
        "Hyperparmeter tuning will then be performed for this best candidate algorithm, again using 'GridSearchCV', but with multiple hyperparameter value combinations."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best algorithm search"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a search class to handle the searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# taken from code-Institute-Solutions/churnometer (https://github.com/Code-Institute-Solutions/churnometer)\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "            model = self.models[key]\n",
        "\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring)\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing parameters for conducting search."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a dictionary of candidate models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
        "\n",
        "models = {'LinearRegression': LinearRegression(),\n",
        "          'DecisionTreeRegressor': DecisionTreeRegressor(random_state=30),\n",
        "          'RandomForestRegressor': RandomForestRegressor(random_state=30),\n",
        "          'ExtraTreeRegressor': ExtraTreeRegressor(random_state=30),\n",
        "          'AdaBoostRegressor': AdaBoostRegressor(random_state=30),\n",
        "          'BaggingRegressor': BaggingRegressor(random_state=30)}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the model parameters for each model; in this case there are no specified parameters meaning the default parameters will be used only as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_model_params = {'LinearRegression': {},\n",
        "                        'DecisionTreeRegressor': {},\n",
        "                        'RandomForestRegressor': {},\n",
        "                        'ExtraTreeRegressor': {},\n",
        "                        'AdaBoostRegressor': {},\n",
        "                        'BaggingRegressor': {}}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applying the data cleaning and engineering pipeline to a copy of the train set features, and scaling a copy of the train target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = x_train.copy(deep=True)\n",
        "y_train_copy = y_train.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_copy = scale_target(y_train_copy, y_train_copy)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = data_cleaning_and_feature_engineering_pipeline.fit(x_train_copy, y_train_copy).transform(x_train_copy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing the search with the created parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models, default_model_params)\n",
        "search.fit(x_train_copy, y_train_copy, scoring='r2', cv=5, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary, grid_search_pipelines = search.score_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All estimators have small or fairly small (<0.1*mean) standard deviations. The best max and mean score was achieved by the 'RandomForestRegressor', closely followed by the 'BaggingRegressor'. The top four estimators all achieved mean scores better than the desired minimum of $R^2=0.75$.\n",
        "\n",
        "All things considered the 'RandomForestRegressor' seems to be the best model candidate. Hyperparameter tuning will now be performed for this model using GridSearchCV with multiple\n",
        "hyperparameter combinations, aided by the use of the HyperparameterOptimizationSearch class.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating MLModel class to handle model tuning and performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLModel():\n",
        "    \"\"\"\n",
        "    Handles candidate model hyperparameter tuning searches, and model performance evaluation.\n",
        "    \n",
        "    For an initialised estimator, train and test dataset, and data cleaning and engineering pipeline,\n",
        "    multiple different tuning searches can be performed using provided model parameters. For each search\n",
        "    the best regressor is identified, and the feature importances for the best fitted model can be extracted.\n",
        "    The best regressor model performance can also be evaluated. All performed searches and other related information\n",
        "    are stored, and are acessible through label indices.\n",
        "\n",
        "    Attributes:\n",
        "        x_train: feature training dataframe.\n",
        "        y_train: target training dataframe.\n",
        "        x_test: feature test dataframe.\n",
        "        y_test: target test dataframe.\n",
        "        cleaning_engineering_pipeline: data cleaning and engineering pipeline instance.\n",
        "        estimator_name: name of the estimator used in the model.\n",
        "        estimator_instance: instance of the estimator used in the model.\n",
        "        indices: list of all performed search indices.\n",
        "        model_params: dictionary of model parameters used in each tuning search performed.\n",
        "        searches: dictionary of search instances used in the tuning searches performed.\n",
        "        best_regressors: dictionary of the best regressor instances identified from the searches performed.\n",
        "        model_pipelines: dictionary of the created model pipelines, created from the identified best regressors.\n",
        "        feature_importances: dictionary of feature importance dataframes, produced from each best fitted model.\n",
        "        model_performances_df: dataframe containing as rows all the performance metrics of each of the best regressors evaluated on the train and test sets.\n",
        "    \"\"\"\n",
        "    def __init__(self, cleaning_engineering_pipeline, estimator_name, estimator_instance, x_train, y_train, x_test, y_test):\n",
        "        self.cleaning_engineering_pipeline = cleaning_engineering_pipeline\n",
        "        self.cleaning_engineering_pipeline.set_output(transform='pandas')\n",
        "        self.estimator_name = estimator_name\n",
        "        self.estimator_instance = estimator_instance\n",
        "        self.indices = []\n",
        "        self.model_params = {}\n",
        "        self.searches = {}\n",
        "        self.best_regressors = {}\n",
        "        self.model_pipelines = {}\n",
        "        self.feature_importances = {}\n",
        "        self.model_performances_df = pd.DataFrame(columns=pd.MultiIndex.from_product([['train', 'test'], ['R2', 'MAE', 'MSE', 'RMSE']]), dtype='float64')\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def transform_training_data(self):\n",
        "        \"\"\"\n",
        "        Fits and transforms the training data using the cleaning_engineering_pipeline property value and the scale target function.\n",
        "\n",
        "        Returns:\n",
        "            The transformed data as a tuple (x_train, y_train).\n",
        "        \"\"\"\n",
        "        y_train = scale_target(self.y_train, self.y_train)[0]\n",
        "        x_train = self.x_train.copy(deep=True)\n",
        "        x_train = self.cleaning_engineering_pipeline.fit(x_train, y_train).transform(x_train)\n",
        "        return (x_train, y_train)\n",
        "    \n",
        "    def tune(self, model_params):\n",
        "        \"\"\"\n",
        "        Performs a hyperparameter tuning 'GridSearchCV' search using the specified model_params.\n",
        "\n",
        "        Performs the search using the training data, pipeline and estimator properties. Stores the search and model_params\n",
        "        by updating the respective properties with a new uniquely indexed entry. Also extracts and stores the best regressor\n",
        "        and its corresponding pipeline and feature importance, by updating the relevant instance properties.\n",
        "\n",
        "        Args:\n",
        "            model_params (dict): 'model_params' parameter for the 'HyperparameterOptimizationSearch' class.   \n",
        "        \"\"\"\n",
        "        search = HyperparameterOptimizationSearch({self.estimator_name: self.estimator_instance}, model_params)\n",
        "        x_train, y_train = self.transform_training_data()\n",
        "        search.fit(x_train, y_train, scoring='r2', cv=5, n_jobs=-1)\n",
        "        grid_search_results_summary, grid_search_pipelines = search.score_summary()\n",
        "        # check uniqueness\n",
        "        if model_params not in self.model_params.values():\n",
        "            index = str(len(self.searches))\n",
        "            # update indices\n",
        "            self.indices.append(index)\n",
        "            # update searches\n",
        "            self.searches.update({index: search})\n",
        "            # update model params\n",
        "            self.model_params.update({index: model_params})\n",
        "            # update best regressors\n",
        "            self.best_regressors.update({index: grid_search_pipelines[self.estimator_name].best_estimator_})\n",
        "            # update pipelines\n",
        "            self.model_pipelines.update({index: Pipeline([(self.estimator_name, self.best_regressors[index])])})\n",
        "            # update feature importance dict\n",
        "            counter = -1\n",
        "            while True:\n",
        "                if hasattr(self.cleaning_engineering_pipeline[counter], 'get_feature_names_out'):\n",
        "                    break\n",
        "                counter -=1\n",
        "            pipeline_features_out = self.cleaning_engineering_pipeline[counter].get_feature_names_out()\n",
        "            regressor_feature_importances = self.best_regressors[index].feature_importances_\n",
        "            feature_importances_df = pd.DataFrame(data=regressor_feature_importances, index=pipeline_features_out, columns=['importance']).sort_values(by='importance', ascending=False)\n",
        "            self.feature_importances.update({index: feature_importances_df})\n",
        "        else:\n",
        "            print('Search performed previously.')\n",
        "    \n",
        "    def extract_feature_importance(self, index=None, print_plot=True):\n",
        "        \"\"\"\n",
        "        Retrieves the feature importance for the best fitted model produced from tuning. It is printed as a barplot/dataframe and returned as a dataframe.\n",
        "\n",
        "        Args:\n",
        "            index (integer): list index of a performed hyperparameter tuning search. Default value is the index of the last performed search.\n",
        "            print_plot (boolean): controls whether the barplot and dataframe are pirnted. Default value True.\n",
        "        \n",
        "        Returns:\n",
        "            A feature importance dataframe for the best fitted model resulting from the stored tuning search matching the index.\n",
        "        \n",
        "        Raises:\n",
        "            KeyError when the index is not found.\n",
        "        \"\"\"\n",
        "        if index == None:\n",
        "            index = len(self.searches) - 1\n",
        "        try:\n",
        "            feature_importances_df = self.feature_importances[str(index)]\n",
        "            if print_plot:\n",
        "                print(f'Feature importance for search index {index}:')\n",
        "                print(feature_importances_df)\n",
        "                # print plot\n",
        "                fig, ax = plt.subplots(tight_layout=True, figsize=(13,5))\n",
        "                ax.set_title('Feature Importance')\n",
        "                sns.barplot(data=feature_importances_df, x=feature_importances_df.index, y=feature_importances_df['importance'], ax=ax)\n",
        "                plt.show()\n",
        "        except KeyError:\n",
        "            print(f'There is no search matching the Index:{index}. Searches with indices in the range 0-{len(self.searches) - 1} currently exist.')\n",
        "        else:\n",
        "            return feature_importances_df\n",
        "\n",
        "    def evaluate_model_performance(self, index=None ):\n",
        "        \"\"\"\n",
        "        Evaluates the performance of the best fitted model produced from the indexed tuning search on the train and test data.\n",
        "        \n",
        "        Prints performance metrics and prediction scatterplots. Updates the model performances dataframe property by adding\n",
        "        the model performance statistics as a new row.\n",
        "\n",
        "        Args:\n",
        "            index (integer): list index of a performed hyperparameter tuning search. Default value is the index of the last performed search.\n",
        "        \n",
        "        Raises:\n",
        "            KeyError when the index is not found.\n",
        "        \"\"\"\n",
        "        if index == None:\n",
        "            index = len(self.searches) - 1\n",
        "        \n",
        "        try:\n",
        "            predictions, train_stats, test_stats = model_evaluation(self.x_train, self.y_train, self.x_test, self.y_test,\n",
        "                                                                    {'data_cleaning_and_feature_engineering': self.cleaning_engineering_pipeline,\n",
        "                                                                    'model': self.model_pipelines[str(index)]})\n",
        "            if index not in self.model_performances_df.index.to_list():\n",
        "                new_entry_df = pd.DataFrame(dtype='float64', index=[index],\n",
        "                                            data={('train', 'R2'): train_stats[0], ('train', 'MAE'): train_stats[1],\n",
        "                                                    ('train', 'MSE'): train_stats[2], ('train', 'RMSE'): train_stats[3],\n",
        "                                                    ('test', 'R2'): test_stats[0], ('test', 'MAE'): test_stats[1],\n",
        "                                                    ('test', 'MSE'): test_stats[2], ('test', 'RMSE'): test_stats[3]})\n",
        "                self.model_performances_df = pd.concat([self.model_performances_df, new_entry_df], ignore_index=False)\n",
        "                self.model_performances_df.sort_values(by=[('test', 'R2')], inplace=True, ascending=False)\n",
        "        except KeyError:\n",
        "            print(f'There is no search matching the Index:{index}. Searches with indices in the range 0-{len(self.searches) - 1} currently exist.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chosen best model candidate hyperparameter tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choosing the model hyperparameter combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 7 hyperparamters that will be tuned:\n",
        "\n",
        "* max_depth\n",
        "* max_leaf_nodes\n",
        "* min_samples_split\n",
        "* min_samples_leaf\n",
        "* n_estimators\n",
        "* max_features\n",
        "* max_samples"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ultimate goal is to avoid under-fitting and over-fitting the train set, leading to either high bias and low variance or low bias but high variance.\n",
        "The other factor to consider is computation cost/time and complexity, with more complex models or higher values of hyperparameters, such as for max_depth, taking a lot more time to compute for the same computation power.\n",
        "\n",
        "Ultimately there is a trade-off that must be decided subject to constraints and the success metric criteria.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of the 7 parameters selected for tuning, some will counteract/limit the effects of each other for certain values. For example increasing max depth, everything else held constant, would lead to more nodes/layers in the tree, provided the min_samples_split/max_leaf_nodes/min_samples_split are not limiting more nodes/layers being formed.\n",
        "\n",
        "What's more many of the parameters possess threshold values, either side of which the model performance on either or the train and test increases, decreases or plateaus.\n",
        "Time permitting ideally a large range (small step-size) of parameters would be trialed and validation curves, for a single or group of parameters, plotted to discover the optimum values."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this project, 3 values will be selected for each parameter and the impact of the various combinations assessed through using GridSearchCV via the HyperparameterOptimizationSearch class."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**max_depth**: the maximum number of layers\n",
        "\n",
        "* As a starting point, the number of features will be used, corresponding to a possible tree structure where each feature is used once (assuming max_features matches) to\n",
        "split a node. Will then take a value, half this, twice this, and two values in between. Hopefully this will indicate the rough location of a threshold value.\n",
        "* So values [5,10,20].\n",
        "\n",
        "**max_leaf_nodes**: the maximum number of terminal nodes. This will influence the number of nodes that can be split.\n",
        "\n",
        "* If every node in every layer is split the number of nodes (n) increases with the depth at a rate $2^{n-1}$. If every node is split ending in leaves for a given depth n,\n",
        "then the number of leaves will be $2^{n}$.\n",
        "* The higher the number of leaf nodes, the more nodes that can be split, the greater the complexity/computation. For n=10 the described structure would have 512 leaves. Also more leaves may lead to over-fitting.\n",
        "* Will cap the leaves at ~25% of this value\n",
        "* So considering the depth values chosen, will take values [32,130,250].\n",
        "\n",
        "**min_samples_split**: minimum number of samples at a node to split. This will counteract the number of leaves and the tree depth.\n",
        "* 1460 samples. Taking the scenario where on average each split equally divides the samples, then it would take $n=log(1460)/log(2)$ levels to give pure leaves, if every node is split. So roughly 11 levels. Again all pure leaves likely leads to overfitting.\n",
        "* So in this scenario lets say leaves have 10% max of the 1460 samples, giving a min_sample_split of 8. Will use this as a starting point.\n",
        "* Will choose values [4,8,64].\n",
        "\n",
        "**min_samples_leaf**: the minimum samples needed for a node to be a leaf.\n",
        "* At worst would want a leaf to have no more than 5% of the samples, and probably not 1 sample either.\n",
        "* Will use values [5,35,65].\n",
        "\n",
        "**n_estimators**: number of trees in the forest. The more the better up to a point where the performance plateaus. However more trees equals greater computation time.\n",
        "* The default value is 100, so will use this as a guide.\n",
        "* Will use values [50,150,250].\n",
        "\n",
        "**max_features**: The max number of features in a random subset to be used in a tree. Again this increases with the number of features up to a point but tails off and decreases.\n",
        "* Apparently a good value for this can be obtained using the sqrt(no. of features).\n",
        "* Also according to the sklearn documentation values close to 100% of the features give good empirical results.\n",
        "* will use values ['sqrt',0.66,1.0].\n",
        "\n",
        "**max_samples**: The sample size of the subset of samples.\n",
        " * Apparently a larger sample size increases the performance, but saturates quickly, and that only a small fraction of the sample is needed generally to achieve this saturation.\n",
        " * Thus will try the values [0.15,0.33,0.5].\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Setting the model parameters using the chosen values.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_params = {'RandomForestRegressor': {\n",
        "    'max_depth': [5,10,20],\n",
        "    'max_samples': [0.15,0.33,0.5],\n",
        "    'max_features': ['sqrt',0.66,1.0],\n",
        "    'n_estimators': [50,150,250],\n",
        "    'min_samples_leaf': [5,35,65],\n",
        "    'min_samples_split': [4,8,64],\n",
        "    'max_leaf_nodes': [32,130,250]\n",
        "}}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Originally attempted to use 5 values for each parameter, but the computation time was far too long."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the MLModel instance for the chosen estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v1 = MLModel(cleaning_engineering_pipeline=data_cleaning_and_feature_engineering(), estimator_name='RandomForestRegressor',\n",
        "                           estimator_instance=RandomForestRegressor(random_state=30), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Performing the hyperparameter tuning search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v1.tune(model_params=model_params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Displaying the top 10 fitted estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary, grid_search_pipelines = random_forest_v1.searches[random_forest_v1.indices[-1]].score_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Retrieving the best hyperparameter combination.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_results_summary.iloc[0, 0]\n",
        "best_model_params = grid_search_pipelines[best_model].best_params_\n",
        "print('Best model:', best_model)\n",
        "print('Best hyperparameter combination:', best_model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v1.best_regressors[random_forest_v1.indices[-1]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Extracting the feature importances**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importances_df = random_forest_v1.extract_feature_importance()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can see that half of the features account for most of the importance, and that the feature 'OverallQual' dominates."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating model on train and test sets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluating the model on the train and test sets**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v1.evaluate_model_performance()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can see from the scatter plots and the result statistics for both the train and test set, that the model predicts well the train and test set target values similarly: the $R^2$ value for the test set is close to that of the train set. The current model, for both the test and train sets achieves $R^2>0.75$, and so the success criterion is met."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Refitting the model with less features**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly the model is biased towards the train set, and has high variance for this test set. Will try to refit the model with fewer features with the hope of reducing the likelihood of over-fitting to the train set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selecting the four features with the highest feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_four_features = feature_importances_df.iloc[0:5,:].index.values.tolist()\n",
        "features_to_drop = x_train.drop(best_four_features, axis=1).columns.tolist()\n",
        "print('Best four features:', best_four_features)\n",
        "print('features to drop:', features_to_drop)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "edit data cleaning and feature engineering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def data_cleaning_and_feature_engineering_v2():\n",
        "    \"\"\"\n",
        "    Constructs and returns data cleaning and feature engineering pipeline.\n",
        "    \"\"\"\n",
        "    # variables for defining pipeline\n",
        "    estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "    # Orginally intended to include the categories parameter used previously for OrdinalEncoder in the\n",
        "    # feature engineering notebook. However causes problems when not all category options are\n",
        "    # present in the train or test set. Will use the 'auto' option instead.\n",
        "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype='int64')\n",
        "    encoder.set_output(transform='pandas')\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.set_output(transform='pandas')\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                        # Data cleaning:\n",
        "                        # Missing value imputation:\n",
        "                        ('IndependentKNNImputer', tf.IndependentKNNImputer()),\n",
        "                        ('EqualFrequencyImputer', tf.EqualFrequencyImputer()),\n",
        "                        #feature engineering:\n",
        "                        # encoding:\n",
        "                        ('OrdinalEncoder', ColumnTransformer(transformers=[('encoder', encoder, ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'])],\n",
        "                                                      remainder='passthrough', n_jobs=-1, verbose_feature_names_out=False)),\n",
        "                        # feature scaling:\n",
        "                        ('Scaling', ColumnTransformer(transformers=[('Power', PowerTransformer(), ['YearBuilt', 'GrLivArea', 'BsmtFinSF1'])],\n",
        "                                                    remainder=min_max_scaler, n_jobs=-1, verbose_feature_names_out=False)),\n",
        "                        # feature number reduction\n",
        "                        ('DropFeatures', DropFeatures(features_to_drop=features_to_drop))\n",
        "                        ])\n",
        "                        \n",
        "    pipeline.set_output(transform='pandas')\n",
        "    return pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating MLModel instance with the new data cleaning and engineering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v2 = MLModel(cleaning_engineering_pipeline=data_cleaning_and_feature_engineering_v2(), estimator_name='RandomForestRegressor',\n",
        "                           estimator_instance=RandomForestRegressor(random_state=30), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redo tuning with best four features only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v2.tune(model_params=model_params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Displaying top 10 fitted estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary_best_four, grid_search_pipelines_best_four = random_forest_v2.searches[random_forest_v2.indices[-1]].score_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary_best_four.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compared to the last search, the scores are lower for this smaller group of more important features. This however is desired to avoid over-fitting and bias towards the train set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Retrieving the best hyperparameter combination.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_results_summary_best_four.iloc[0, 0]\n",
        "best_model_params = grid_search_pipelines_best_four[best_model].best_params_\n",
        "print('Best model:', best_model)\n",
        "print('Best hyperparameter combination:', best_model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v2.best_regressors[random_forest_v2.indices[-1]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Extracting the feature importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importances_best_four_df = random_forest_v2.extract_feature_importance()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 'OverallQual' feature still dominates, with the remaining features having a combined importance value of about ~40%. Does seem to suggest that 'OverallQual' is the most significant feature for the sale price prediction; which was predicted during the sale price correlation study. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating model on the train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v2.evaluate_model_performance()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen from the plots and the $R^2$ values that the model has significantly improved in predicting the test set target values, going from $R^2\\approx0.49$ to $R^2\\approx0.74$.\n",
        "At the same the train set performance has declined a fair bit to $R^2\\approx0.75$, but as mentioned before this is a consequence of reducing the degree of over-fitting. \n",
        "\n",
        "Despite the improvement in performance on the test set, the model performance is marginally below the success criterion of $R^2=0.75$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Possible next steps**:\n",
        "* Try an even smaller group of features, or a different combination of the most important features.\n",
        "* Perform hyperparameter tuning with a new range of hyperparamter values.\n",
        "* Make adjustments to the data cleaning and feature engineering pipeline."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Refitting with 3 most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_three_features = feature_importances_df.iloc[0:3,:].index.values.tolist()\n",
        "features_to_drop = x_train.drop(best_three_features, axis=1).columns.tolist()\n",
        "print('Best three features:', best_three_features)\n",
        "print('features to drop:', features_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "import src.ml.transformers_and_functions as tf\n",
        "\n",
        "\n",
        "def data_cleaning_and_feature_engineering_v3():\n",
        "    \"\"\"\n",
        "    Constructs and returns data cleaning and feature engineering pipeline.\n",
        "    \"\"\"\n",
        "    # variables for defining pipeline\n",
        "    estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "    # Orginally intended to include the categories parameter used previously for OrdinalEncoder in the\n",
        "    # feature engineering notebook. However causes problems when not all category options are\n",
        "    # present in the train or test set. Will use the 'auto' option instead. \n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                        # Data cleaning:\n",
        "                        # Missing value imputation:\n",
        "                        ('IndependentKNNImputer', tf.IndependentKNNImputer()),\n",
        "                        ('EqualFrequencyImputer', tf.EqualFrequencyImputer()),\n",
        "                        #feature engineering:\n",
        "                        # encoding:\n",
        "                        ('OrdinalEncoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype='int64')),\n",
        "                        # feature number reduction\n",
        "                        ('DropFeatures', DropFeatures(features_to_drop=features_to_drop)),\n",
        "                        # feature scaling:\n",
        "                        ('CompositeNormaliser', tf.CompositeNormaliser())\n",
        "                        ])\n",
        "    return pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating MLModel instance with the new data cleaning and engineering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v3 = MLModel(cleaning_engineering_pipeline=data_cleaning_and_feature_engineering_v3(), estimator_name='RandomForestRegressor',\n",
        "                           estimator_instance=RandomForestRegressor(random_state=30), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redo tuning with only the 3 best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v3.tune(model_params=model_params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Displaying the top 10 fitted estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary_best_three, grid_search_pipelines_best_three = random_forest_v3.searches[random_forest_v3.indices[-1]].score_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary_best_three.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The validation scores of the best regressors are worse than for the previous four most important feature model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_results_summary_best_three.iloc[0, 0]\n",
        "best_model_params = grid_search_pipelines_best_three[best_model].best_params_\n",
        "print('Best model:', best_model)\n",
        "print('Best hyperparameter combination:', best_model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v3.best_regressors[random_forest_v3.indices[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importances_best_three_df = random_forest_v3.extract_feature_importance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating model on the train and test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v3.evaluate_model_performance()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The $R^2$ value is slightly worse for both the test and train set relative to the previous four most important feature model. Thus it appears reducing the number of features in the model further does not improve the model performance."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Studying the validation curves"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the current best 4 feature model will look at the validation curves for each hyperparameter to see how altering the parameters further may or may not significantly\n",
        "improve the model performance.\n",
        "\n",
        "Of course a limitation inherent to varying one parameter whilst keep others fixed, is that the degree of variation in model performance is dependent on the values of\n",
        "the fixed parameters. The model performance is a multivariable function of the hyperparameters, that can be visualised as a multidimensional surface whose height is the\n",
        "model performance score. Constraining all but one parameter effectively reduces movement along the surface to a single contour for which the hyperparameter values are constant. The exact contour will vary with the values of the fixed parameters. As such a global maximum in the surface height may not be discovered unless the contour passes through the surface at this point. Nonetheless it is hoped that the previous grid search tuning means that the hyperparameter values are close to their optimal values, such that the contour does pass through any global maximum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_four_features = feature_importances_df.iloc[0:4,:].index.values.tolist()\n",
        "features_to_drop = x_train.drop(best_four_features, axis=1).columns.tolist()\n",
        "print('Best four features:', best_four_features)\n",
        "print('features to drop:', features_to_drop)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reminder of the current best regressor parameters for the 4 feature model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v2.best_regressors['0']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a validation curve for the max_depth parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = x_train.copy(deep=True)\n",
        "y_train_copy = y_train.copy(deep=True)\n",
        "y_train_copy = scale_target(y_train_copy, y_train_copy)[0]\n",
        "x_train_copy = random_forest_v2.cleaning_engineering_pipeline.fit(x_train_copy, y_train_copy).transform(x_train_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import validation_curve\n",
        "train_scores, validation_scores = validation_curve(estimator=random_forest_v2.best_regressors['0'], X=x_train_copy, y=y_train_copy, param_name='max_depth',\n",
        "                                             param_range=np.arange(1,25,1), cv=5, scoring='r2', n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "x = np.arange(1,25,1)\n",
        "y1 = np.mean(train_scores, axis=1)\n",
        "y2 = np.mean(validation_scores, axis=1)\n",
        "ax.set_xticks(x)\n",
        "ax.set(title='Validation curve for max_depth parameter')\n",
        "ax.plot(x, y1, marker='o', label='training score')\n",
        "ax.plot(x, y2, marker='o', label='validation score')\n",
        "ax.legend(loc='center')\n",
        "ax.fill_between(x, y1 + np.std(train_scores, axis=1), y1 - np.std(train_scores, axis=1), alpha=0.4)\n",
        "ax.fill_between(x, y2 + np.std(validation_scores, axis=1), y2 - np.std(validation_scores, axis=1), alpha=0.4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training and validation scores are fairly similar, but there is some overfitting. The model performance plateaus after a max depth value of 6."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a validation curve for the min_samples_leaf parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_scores, validation_scores = validation_curve(estimator=random_forest_v2.best_regressors['0'], X=x_train_copy, y=y_train_copy, param_name='min_samples_leaf',\n",
        "                                             param_range=np.arange(1,30,1), cv=5, scoring='r2', n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(9,9))\n",
        "x = np.arange(1,30,1)\n",
        "y1 = np.mean(train_scores, axis=1)\n",
        "y2 = np.mean(validation_scores, axis=1)\n",
        "ax.set_xticks(x)\n",
        "ax.set(title='Validation curve for min_samples_leaf parameter')\n",
        "ax.plot(x, y1, marker='o', label='training score')\n",
        "ax.plot(x, y2, marker='o', label='validation score')\n",
        "ax.legend(loc='upper right')\n",
        "ax.fill_between(x, y1 + np.std(train_scores, axis=1), y1 - np.std(train_scores, axis=1), alpha=0.4)\n",
        "ax.fill_between(x, y2 + np.std(validation_scores, axis=1), y2 - np.std(validation_scores, axis=1), alpha=0.4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training and validation scores are fairly similar for all but small values where there is some overfitting. The model performance seems to decrease after the\n",
        "min_samples_leaf values of 2 or less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a validation curve for the min_samples_split parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_scores, validation_scores = validation_curve(estimator=random_forest_v2.best_regressors['0'], X=x_train_copy, y=y_train_copy, param_name='min_samples_split',\n",
        "                                             param_range=np.arange(2,101,2), cv=5, scoring='r2', n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,7), tight_layout=True)\n",
        "x = np.arange(2,101,2)\n",
        "y1 = np.mean(train_scores, axis=1)\n",
        "y2 = np.mean(validation_scores, axis=1)\n",
        "ax.set_xticks(x)\n",
        "ax.set(title='Validation curve for min_samples_split parameter')\n",
        "ax.plot(x, y1, marker='o', label='training score')\n",
        "ax.plot(x, y2, marker='o', label='validation score')\n",
        "ax.legend(loc='upper center')\n",
        "ax.fill_between(x, y1 + np.std(train_scores, axis=1), y1 - np.std(train_scores, axis=1), alpha=0.4)\n",
        "ax.fill_between(x, y2 + np.std(validation_scores, axis=1), y2 - np.std(validation_scores, axis=1), alpha=0.4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training and validation scores are again fairly similar, but there is some overfitting for smaller values. The model performance decreases after a min_samples_split value of around 16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a validation curve for the max_leaf_nodes parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_scores, validation_scores = validation_curve(estimator=random_forest_v2.best_regressors['0'], X=x_train_copy, y=y_train_copy, param_name='max_leaf_nodes',\n",
        "                                             param_range=np.arange(0, 151, 5), cv=5, scoring='r2', n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(13,7))\n",
        "x = np.arange(0, 151, 5)\n",
        "y1 = np.mean(train_scores, axis=1)\n",
        "y2 = np.mean(validation_scores, axis=1)\n",
        "ax.set_xticks(x)\n",
        "ax.set(title='Validation curve for max_leaf_nodes parameter')\n",
        "ax.plot(x, y1, marker='o', label='training score')\n",
        "ax.plot(x, y2, marker='o', label='validation score')\n",
        "ax.legend(loc='center')\n",
        "ax.fill_between(x, y1 + np.std(train_scores, axis=1), y1 - np.std(train_scores, axis=1), alpha=0.4)\n",
        "ax.fill_between(x, y2 + np.std(validation_scores, axis=1), y2 - np.std(validation_scores, axis=1), alpha=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training and validation scores are fairly similar only for very small values, after there is noticeable overfitting. The model performance plateaus after a max_leaf_nodes\n",
        "value of around 25."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a validation curve for the max_samples parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_scores, validation_scores = validation_curve(estimator=random_forest_v2.best_regressors['0'], X=x_train_copy, y=y_train_copy, param_name='max_samples',\n",
        "                                             param_range=np.arange(0.1, 1.05, 0.05), cv=5, scoring='r2', n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(9,7))\n",
        "x = np.arange(0.1, 1.05, 0.05)\n",
        "y1 = np.mean(train_scores, axis=1)\n",
        "y2 = np.mean(validation_scores, axis=1)\n",
        "ax.set_xticks(x)\n",
        "ax.set(title='Validation curve for max_samples parameter')\n",
        "ax.plot(x, y1, marker='o', label='training score')\n",
        "ax.plot(x, y2, marker='o', label='validation score')\n",
        "ax.legend(loc='upper left')\n",
        "ax.fill_between(x, y1 + np.std(train_scores, axis=1), y1 - np.std(train_scores, axis=1), alpha=0.4)\n",
        "ax.fill_between(x, y2 + np.std(validation_scores, axis=1), y2 - np.std(validation_scores, axis=1), alpha=0.4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training and validation scores are fairly similar again only for small values, and after there is some overfitting. The model performance is slowly plateaus after a value of 0.3 for the validation score, whilst unsurprisingly it continues to increases for the training score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a validation curve for the n_estimators parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_scores, validation_scores = validation_curve(estimator=random_forest_v2.best_regressors['0'], X=x_train_copy, y=y_train_copy, param_name='n_estimators',\n",
        "                                             param_range=np.arange(10, 351, 10), cv=5, scoring='r2', n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(13,7), tight_layout=True)\n",
        "x = np.arange(10, 351, 10)\n",
        "y1 = np.mean(train_scores, axis=1)\n",
        "y2 = np.mean(validation_scores, axis=1)\n",
        "ax.set_xticks(x)\n",
        "ax.set(title='Validation curve for n_estimators parameter')\n",
        "ax.plot(x, y1, marker='o', label='training score')\n",
        "ax.plot(x, y2, marker='o', label='validation score')\n",
        "ax.legend(loc='lower right')\n",
        "ax.fill_between(x, y1 + np.std(train_scores, axis=1), y1 - np.std(train_scores, axis=1), alpha=0.4)\n",
        "ax.fill_between(x, y2 + np.std(validation_scores, axis=1), y2 - np.std(validation_scores, axis=1), alpha=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is significant overfitting. The model performance plateaus after a value of 150."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating a validation curve for the max_features parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_scores, validation_scores = validation_curve(estimator=random_forest_v2.best_regressors['0'], X=x_train_copy, y=y_train_copy, param_name='max_features',\n",
        "                                             param_range=[1,2,3,4], cv=5, scoring='r2', n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7,7), tight_layout=True)\n",
        "x = [1,2,3,4]\n",
        "y1 = np.mean(train_scores, axis=1)\n",
        "y2 = np.mean(validation_scores, axis=1)\n",
        "ax.set_xticks(x)\n",
        "ax.set(title='Validation curve for max_features parameter')\n",
        "ax.plot(x, y1, marker='o', label='training score')\n",
        "ax.plot(x, y2, marker='o', label='validation score')\n",
        "ax.legend(loc='upper left')\n",
        "ax.fill_between(x, y1 + np.std(train_scores, axis=1), y1 - np.std(train_scores, axis=1), alpha=0.4)\n",
        "ax.fill_between(x, y2 + np.std(validation_scores, axis=1), y2 - np.std(validation_scores, axis=1), alpha=0.4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is significant overfitting. The model performance peaks for a value of 2 for the validation set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General comments\n",
        "- For most validation curves, the validation scores have a large degree of variance for all parameter values.\n",
        "- All curves plateau/peak quickly, and only a few decline rapidly.\n",
        "- Also the current best model parameter values achieve close to the highest scoring values for most parameters, however for the parameters max_samples, min_samples_split, max_leaf_nodes, and min_samples_leaf, some improvements to the model performance may be possible with slight tweaks to these parameter values.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Changing one of these parameters will change the location on the hyperparameter surface, and so the improvements implied by tweaking the remaining parameters may no longer be the same. Thus rather than just changing all parameters simultaneously, a small Grid Search tuning will be performed with a mixture of the original and tweaked parameters."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation curve inspired further grid search tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating MLModel instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v4 = MLModel(cleaning_engineering_pipeline=data_cleaning_and_feature_engineering_v2(), estimator_name='RandomForestRegressor',\n",
        "                           estimator_instance=RandomForestRegressor(random_state=30), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_params = {'RandomForestRegressor': {\n",
        "    'max_depth': [10],\n",
        "    'max_samples': [0.5,1.0],\n",
        "    'max_features': ['sqrt'],\n",
        "    'n_estimators': [150],\n",
        "    'min_samples_leaf': [2,5],\n",
        "    'min_samples_split': [2,4,12],\n",
        "    'max_leaf_nodes': [25,32]\n",
        "}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v4.tune(model_params=model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary, grid_search_pipelines  = random_forest_v4.searches[random_forest_v4.indices[-1]].score_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems tweaking the max_samples and min_samples_leaf parameters is responsible for an improved best regressor validation set score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_results_summary.iloc[0, 0]\n",
        "best_model_params = grid_search_pipelines[best_model].best_params_\n",
        "print('Best model:', best_model)\n",
        "print('Best hyperparameter combination:', best_model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v4.best_regressors['0']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating model on the train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v4.evaluate_model_performance()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen from the plots and the $R^2$ values that the model has slightly improved in predicting the test set target values, going from $R^2\\approx0.741$ to $R^2\\approx0.751$.\n",
        "At the same the train set performance has improved a bit to $R^2\\approx0.79$. \n",
        "\n",
        "With this improvement in performance, the model performance on both the train and test set meets the success criterion of $R^2>0.75$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Current best regressor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best regressor is currently the model fitted with the 4 most important features, with a test set score of $R^2=0.751$. This meets the success criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v4.best_regressors['0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_results_summary.iloc[0, 0]\n",
        "best_model_params = grid_search_pipelines[best_model].best_params_\n",
        "print('Best model:', best_model)\n",
        "print('Best hyperparameter combination:', best_model_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With feature importance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v4.extract_feature_importance(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And model performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_forest_v4.evaluate_model_performance(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving the current best pipelines, feature importances df, and the model performances df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_pipeline = random_forest_v4.model_pipelines['0']\n",
        "data_cleaning_and_feature_engineering_pipeline = random_forest_v4.cleaning_engineering_pipeline\n",
        "feature_importances_df = random_forest_v4.extract_feature_importance(print_plot=False)\n",
        "model_performances_df = random_forest_v4.model_performances_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    joblib.dump(value=model_pipeline, filename=f\"{file_path}/model_pipeline.pkl\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    joblib.dump(value=data_cleaning_and_feature_engineering_pipeline, filename=f\"{file_path}/data_cleaning_and_feature_engineering_pipeline.pkl\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    feature_importances_df.to_csv(f\"{file_path}/feature_importances_df.csv\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    model_performances_df.to_csv(f\"{file_path}/model_performances_df.csv\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    train_set_df.to_csv(f\"{file_path}/train_set_df.csv\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    test_set_df.to_csv(f\"{file_path}/test_set_df.csv\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
