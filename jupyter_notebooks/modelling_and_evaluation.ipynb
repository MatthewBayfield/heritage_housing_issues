{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Modelling and Evaluation**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "**Perform Business requirement 2 user story tasks: model selection, pipeline creation, hyperparameter tuning, model evaluation.**\n",
        "* Create initial data cleaning and engineering pipeline using information from previous notebooks.\n",
        "* Create initial modelling and evaluation pipeline using information from previous notebooks.\n",
        "* Find best model candidate.\n",
        "* Optimise chosen model through tuning and feature selection using feature importance. \n",
        "* Evaluate the model performance using performance metrics.\n",
        "* Successfully achieve $R^2 \\ge 0.75$ for the final model, to satisfy the client's success criteria, and thereby satisfy business requirement 2.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "* house prices dataset: outputs/datasets/collection/house_prices.csv.\n",
        "* Information regarding the steps to include in the various pipelines, as indicated in the conclusion sections of the data cleaning and feature engineering notebooks.\n",
        "* Outlier indices list: outputs/ml/outlier_indices.pkl\n",
        "\n",
        "## Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "Working directory changed to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load house price dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "house_prices_df = pd.read_csv(filepath_or_buffer='outputs/datasets/collection/house_prices.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing known outliers from the whole dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading outlier indices list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "outlier_indices = joblib.load('outputs/ml/outlier_indices.pkl')\n",
        "outlier_indices"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing the instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_prices_df.drop(labels=outlier_indices, inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create data cleaning and feature engineering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import src.transformers_and_functions as tf\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
        "from feature_engine.selection import SmartCorrelatedSelection, DropFeatures\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_cleaning_and_feature_engineering():\n",
        "    \"\"\"\n",
        "    Constructs and returns data cleaning and feature engineering pipeline.\n",
        "    \"\"\"\n",
        "    # variables for defining pipeline\n",
        "    estimator = DecisionTreeRegressor(min_samples_split=10, min_samples_leaf=5, random_state=30)\n",
        "    # Orginally intended to include the categories parameter used previously for OrdinalEncoder in the\n",
        "    # feature engineering notebook. However causes problems when not all category options are\n",
        "    # present in the train or test set. Will use the 'auto' option instead. \n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                        # Data cleaning:\n",
        "                        # Missing value imputation:\n",
        "                        ('IndependentKNNImputer', tf.IndependentKNNImputer()),\n",
        "                        ('EqualFrequencyImputer', tf.EqualFrequencyImputer()),\n",
        "                        #feature engineering:\n",
        "                        # encoding:\n",
        "                        ('OrdinalEncoder', OrdinalEncoder(dtype='int64')),\n",
        "                        # feature number reduction\n",
        "                        ('CompositeSelectKBest', tf.CompositeSelectKBest()),\n",
        "                        ('SmartCorrelatedSelection', SmartCorrelatedSelection(method='spearman',\n",
        "                                                                              threshold=0.8, selection_method='model_performance',\n",
        "                                                                              estimator=estimator, scoring='r2', cv=5)),\n",
        "                        # #feature scaling:\n",
        "                        #('CompositeNormaliSer', tf.CompositeNormaliser())\n",
        "                        ])\n",
        "    return pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As commented inside the data cleaning and feature engineering function above, it was intended to manually specify the ordinal encoding mapping using ordered arrays, as was done in the feature engineering notebook. However the train and test sets might not have all feature options for all features. For example for the train set the feature 'KitchenQual' does not have the value 'Po'. Therefore the encoding will be done automatically, and consequently the natural ranking of the feature values for a feature may not be preserved. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_cleaning_and_feature_engineering_pipeline = data_cleaning_and_feature_engineering()\n",
        "data_cleaning_and_feature_engineering_pipeline.set_output(transform='pandas')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_set_df, test_set_df) = train_test_split(house_prices_df, test_size=0.25, random_state=30)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the train and test sets in to their features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = train_set_df.drop('SalePrice', axis=1)\n",
        "y_train = train_set_df['SalePrice']\n",
        "x_test = test_set_df.drop('SalePrice', axis=1)\n",
        "y_test = test_set_df['SalePrice']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Scale target function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scale_target(y_train, y_test=None):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    y_train = pd.DataFrame(data=y_train)\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    min_max_scaler.set_output(transform='pandas')\n",
        "    min_max_scaler.fit(y_train)\n",
        "\n",
        "    y_train = min_max_scaler.transform(y_train)\n",
        "\n",
        "    if y_test:\n",
        "        y_test = min_max_scaler.transform(y_test)\n",
        "        return y_train, y_test\n",
        "\n",
        "    return y_train.iloc[:, 0]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Grid Search CV"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initially a search will be done to find the most suitable algorithm using sklearn's 'GridSearchCV', using only the default hyperparameters for each algorithm.\n",
        "Hyperparmeter tuning will then be performed for this best candidate algorithm, again using 'GridSearchCV', but with multiple hyperparameter value combinations."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best algorithm search"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a search class to handle the searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# taken from code-Institute-Solutions/churnometer (https://github.com/Code-Institute-Solutions/churnometer)\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "            model = self.models[key]\n",
        "\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring)\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing parameters for conducting search."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a dictionary of candidate models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
        "\n",
        "models = {'LinearRegression': LinearRegression(),\n",
        "          'DecisionTreeRegressor': DecisionTreeRegressor(random_state=30),\n",
        "          'RandomForestRegressor': RandomForestRegressor(random_state=30),\n",
        "          'ExtraTreeRegressor': ExtraTreeRegressor(random_state=30),\n",
        "          'AdaBoostRegressor': AdaBoostRegressor(random_state=30),\n",
        "          'BaggingRegressor': BaggingRegressor(random_state=0)}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the model parameters for each model; in this case there are no specified parameters meaning the default parameters will be used only as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_model_params = {'LinearRegression': {},\n",
        "                        'DecisionTreeRegressor': {},\n",
        "                        'RandomForestRegressor': {},\n",
        "                        'ExtraTreeRegressor': {},\n",
        "                        'AdaBoostRegressor': {},\n",
        "                        'BaggingRegressor': {}}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applying the data cleaning and engineering pipeline to a copy of the train set features, and scaling a copy of the train target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = x_train.copy(deep=True)\n",
        "y_train_copy = y_train.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_copy = data_cleaning_and_feature_engineering_pipeline.fit(x_train_copy, y_train_copy).transform(x_train_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_copy = scale_target(y_train.copy(deep=True))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing the search with the created parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models, default_model_params)\n",
        "search.fit(x_train_copy, y_train_copy, scoring='r2', cv=5, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary, grid_search_pipelines = search.score_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_results_summary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All estimators have small or fairly small (<0.1*mean) standard deviations. The best max and mean score was achieved by the 'RandomForestRegressor', closely followed by the 'BaggingRegressor'. The top four estimators all achieved min scores better than the desired minimum of $R^2=0.75$.\n",
        "\n",
        "All things considered the 'RandomForestRegressor' seems to be the best model candidate. Hyperparameter tuning will now be performed for this model using GridSearchCV with multiple\n",
        "hyperparameter combinations, aided by the use of the HyperparameterOptimizationSearch class.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
