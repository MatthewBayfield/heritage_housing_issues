{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **How house attributes influence the sale price**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "**Generate content to satisfy business requirement 1**:\n",
        "* Determine how features are correlated to the target, and thus their significance in determining the sale price.\n",
        "\n",
        "**Business requirement 2**:\n",
        "* Determine which features are significant enough, with regard to predicting the sale price, to be studied in the exploratory data analysis notebook.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* house prices dataset: outputs/datasets/collection/house_prices.csv\n",
        "\n",
        "## Outputs\n",
        "**For use on the dashboard**:\n",
        "* Code for calculating Spearman, phi_k correlation coefficients, significance values and PPS's.\n",
        "* Code to produce heatmaps for the coefficients and PPS's.\n",
        "* Code to produce scatter plots for feature-target pairs.\n",
        "* Cleaned dataset for correlation tests: outputs/datasets/sale_price_study/cleaned/house_prices.csv\n",
        "* categorical feature encoded dataset: outputs/datasets/sale_price_study/cleaned/encoded_house_prices.csv\n",
        "* pickled selected significant features list: outputs/selected_significant_features.pkl\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "Working directory changed to its parent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "os.getcwd()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load house prices dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "house_prices_df = pd.read_csv(filepath_or_buffer='outputs/datasets/collection/house_prices.csv')\n",
        "house_prices_df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dealing with missing data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discover columns with missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_data_df = house_prices_df.loc[:, house_prices_df.isna().any()]\n",
        "print(missing_data_df.info())\n",
        "house_prices_numeric_df = house_prices_df.select_dtypes(exclude=['object']).drop(['SalePrice'], axis=1)\n",
        "print(house_prices_numeric_df.columns)\n",
        "house_prices_non_numeric_df = house_prices_df.select_dtypes(include='object')\n",
        "house_prices_non_numeric_df.columns\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Impute missing values in numeric columns**.  \n",
        "Want to choose an impute method that does not distort the distribution significantly, whilst preserving any existing correlations.\n",
        "With this in mind the KNNImputer is employed with 5 nearest neighbours used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plotting distributions for numeric variables with missing data\n",
        "counter = 0\n",
        "imputed_columns = []\n",
        "while counter < len(house_prices_numeric_df.columns):\n",
        "    if house_prices_numeric_df.iloc[:, counter].name in missing_data_df.columns:\n",
        "        fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(10,4))\n",
        "        sns.histplot(x=house_prices_numeric_df.iloc[:, counter], ax=ax[0])\n",
        "        imputed_columns.append(house_prices_numeric_df.iloc[:, counter].name)\n",
        "    counter += 1\n",
        "\n",
        "# Imputing the missing values for required columns\n",
        "imputer = KNNImputer()\n",
        "imputer.set_output(transform='pandas')\n",
        "house_prices_numeric_df = imputer.fit_transform(house_prices_numeric_df)\n",
        "print(house_prices_numeric_df.isna().sum())\n",
        "house_prices_df[house_prices_numeric_df.columns.values] = house_prices_numeric_df\n",
        "\n",
        "for col in ['BedroomAbvGr','GarageYrBlt','OverallCond','OverallQual','YearBuilt', 'YearRemodAdd']:\n",
        "    house_prices_numeric_df[col] = house_prices_numeric_df[col].round()\n",
        "\n",
        "# plotting distributions after imputation on same figures, for visual comparison\n",
        "for fig in plt.get_fignums():\n",
        "    sns.histplot(x=house_prices_numeric_df[imputed_columns[fig - 1]], ax=plt.figure(fig).get_axes()[1])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspection of the above figures showing the variable distributions before and after imputation, shows that apart from the 'EnclosedPorchSF' and 'WoodDeckSF' variables,\n",
        "there is no noticeable change to the distribution shapes. In the case of the EnclosedPorchSF variable, a large quantity of data was missing,\n",
        "so the somewhat noticeable change is inevitable; the same is true for the WoodDeckSF variable."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Impute missing values in non-numeric columns**.  \n",
        "In the absence of a viable distance metric, and considering the fact that only a small amount of data is missing, the KNNImputer was not used. Instead the more\n",
        "simple method for filling the values by cycling through the possible values was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_data_non_numeric_df = missing_data_df.select_dtypes(include='object')\n",
        "print(missing_data_non_numeric_df.columns)\n",
        "\n",
        "# plotting the variable distributions before imputation\n",
        "counter = 0\n",
        "imputed_columns = []\n",
        "while counter < len(missing_data_non_numeric_df.columns):\n",
        "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(10,4))\n",
        "    sns.countplot(x=missing_data_non_numeric_df.iloc[:, counter], ax=ax[0])\n",
        "    imputed_columns.append(missing_data_non_numeric_df.iloc[:, counter].name)\n",
        "    counter += 1\n",
        "\n",
        "# imputing the missing values\n",
        "for col in imputed_columns:\n",
        "    number_of_nans = house_prices_df[col].loc[(house_prices_df[col].isna() == True)].size\n",
        "    unique_values = house_prices_df[col].unique()\n",
        "    index_no = 0\n",
        "    while number_of_nans > 0:\n",
        "        if index_no + 1 >= unique_values.size:\n",
        "            index_no = 0\n",
        "        house_prices_df[col].fillna(value=unique_values[index_no], limit=1, inplace=True)\n",
        "        index_no += 1\n",
        "        number_of_nans = house_prices_df[col].loc[(house_prices_df[col].isna() == True)].size\n",
        "\n",
        "# plotting the distributions on the same figures after imputation for comparison\n",
        "for fig in plt.get_fignums():\n",
        "    sns.countplot(x=house_prices_df[imputed_columns[fig - 1]], ax=plt.figure(fig).get_axes()[1])\n",
        "          \n",
        "house_prices_df.isna().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can see from the above figures that there are no significant changes in the distribution shapes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save cleaned dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Needed for the dashboard, for the sale price correlation study page code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    path = os.path.join(os.getcwd(), 'outputs/datasets/sale_price_study/cleaned')\n",
        "    os.makedirs(path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    house_prices_df.to_csv(os.path.join(path, 'house_prices.csv'), index=False)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation matrices for features and target: sale price"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normality test for sale price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pingouin as pg\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, figsize=(10,10))\n",
        "sns.histplot(x=house_prices_df['SalePrice'], kde=True, ax=axes[0])\n",
        "pg.qqplot(x=house_prices_df['SalePrice'], ax=axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('skew:', house_prices_df['SalePrice'].skew())\n",
        "print('kurtosis:', house_prices_df['SalePrice'].kurtosis())\n",
        "pg.normality(house_prices_df['SalePrice'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examining the sale price histogram, QQ-plot, values of the skew and kutosis, as well as the Shapiro-Wilk normality test, shows that the sale price is not normally distributed.\n",
        "It is also known that many of the features are not normally distributed and may have possible outliers; as such considering the value-time tradeoff, the pearson correlation test will not be used. To use it would require an attempt to transform the distributions in order to normalise them (if possible), as well as remove any outliers."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Instead the Spearman and phi_k correlation coefficients will be calculated**."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spearman"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Categorical variable encoding**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_prices_categorical_df = house_prices_df.select_dtypes(include='object')\n",
        "house_prices_categorical_df.columns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All categorical variables are ordinal, therefore use ordinal encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "bsmt_fin_type1_cat = np.array(list(reversed(['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'])))\n",
        "bsmt_exposure_cat = np.array(['None', 'No', 'Mn', 'Av', 'Gd'])\n",
        "garage_finish_cat = np.array(['None', 'Unf', 'RFn', 'Fin'])\n",
        "kitchen_quality_cat = np.array(['Po', 'Fa', 'TA', 'Gd', 'Ex'])\n",
        "\n",
        "categories = [bsmt_exposure_cat, bsmt_fin_type1_cat, garage_finish_cat, kitchen_quality_cat]\n",
        "encoder = OrdinalEncoder(categories=categories, dtype='int64')\n",
        "encoder.set_output(transform='pandas')\n",
        "\n",
        "\n",
        "house_prices_df[house_prices_categorical_df.columns] = encoder.fit_transform(X=house_prices_categorical_df)\n",
        "house_prices_df[house_prices_categorical_df.columns].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    path = os.path.join(os.getcwd(), 'outputs/datasets/sale_price_study/cleaned')\n",
        "    os.makedirs(path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    house_prices_df.to_csv(os.path.join(path, 'encoded_house_prices.csv'), index=False)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When assessing and describing correlation coefficients the following definitions will be used:  \n",
        "r value range|Description\n",
        "|:----|:----|\n",
        "|0-0.34|weak|\n",
        "|0.35-0.64|moderate|\n",
        "|0.65-1|strong|\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chosen significance level value: 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spearman_df = house_prices_df.pairwise_corr(columns=['SalePrice'], alternative='greater', method='spearman')\n",
        "spearman_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the spearman_df dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    path = os.path.join(os.getcwd(), 'src/sale_price_study/')\n",
        "    os.makedirs(path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    spearman_df.to_csv(os.path.join(path, 'spearman_df.csv'), index=False)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "spearman_heatmap = sns.heatmap(spearman_df.pivot(index='Y', columns=['X'], values=['r']).sort_values(by=('r', 'SalePrice'), ascending=False), annot=True,\n",
        "                                              vmax=1, vmin=-1, xticklabels=['SalePrice'], linecolor='black', linewidth=0.05)\n",
        "spearman_heatmap.set(xlabel='', ylabel='Feature', title='SalePrice-Feature pair spearman correlations')\n",
        "spearman_heatmap"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation of the coefficients and their associated p values implies the following outcomes with regard to accepting the common alternative hypotheses that the correlation coefficients are greater than zero or equivalently the correlations are positive monotonic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'|Feature|Null hypothesis (r≤0) outcome|')\n",
        "for feature_name in house_prices_df.columns:\n",
        "    if feature_name not in ['OverallCond', 'EnclosedPorchSF', 'SalePrice']:\n",
        "        print(f'{feature_name}:', 'reject' )\n",
        "    elif feature_name in ['OverallCond', 'EnclosedPorchSF']:\n",
        "        print(f'{feature_name}:', 'accept' )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Therefore the Size group and Quality group (alternative) hypotheses, can be accepted: namely there exists statistically significant positive monotonic correlations between those features and the sale price. The same is true for the Age/Condition group and feature group 4, except for\n",
        "the features 'OverallCond' and 'EnclosedPorch', where the correlation has a negative value, and could well be zero as indicated by the 95% confidence intervals."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With regard to strength as defined earlier, the correlations for each feature can be described as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'|Feature|Strength|')\n",
        "strong_features = []\n",
        "moderate_features = []\n",
        "weak_features = []\n",
        "for feature_name, r_value in spearman_df[['Y', 'r']].set_index('Y').iterrows():\n",
        "    strength = ''\n",
        "    if abs(r_value.values[0]) > 0.65:\n",
        "        strength = 'strong'\n",
        "        strong_features.append((feature_name, r_value))\n",
        "    elif abs(r_value.values[0]) > 0.35:\n",
        "        strength = 'moderate'\n",
        "        moderate_features.append((feature_name, r_value))\n",
        "    elif abs(r_value.values[0]) > 0:\n",
        "        strength = 'weak'\n",
        "        weak_features.append((feature_name, r_value))\n",
        "\n",
        "for feature_tuple in strong_features:\n",
        "    print(f'{feature_tuple[0]}:', 'strong')\n",
        "for feature_tuple in moderate_features:\n",
        "    print(f'{feature_tuple[0]}:', 'moderate')\n",
        "for feature_tuple in weak_features:\n",
        "    print(f'{feature_tuple[0]}:', 'weak')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Only 'GrLivArea' of the size group is as predicted; all the basement related features (except for 'TotalBsmtSF) expressed weak correlations with the sale price; the lot based features have moderate correlations. More surprisingly the '2ndFlrSF' and 'BedroomAbvGrade' features are only weakly correlated to the sale price; in the case of '2ndFlrSf', this may be because of the large number of instances with the value 0 impacting the r coefficient value.\n",
        "* For the quality group both 'KitchenQual' and 'OverallQual' are strongly correlated as predicted.  \n",
        "* For the Age/Condition group, only 'YearRemodAdd' is as expected, with 'YearBuilt' being strongly correlated, and 'OverallCond being weakly correlated with sale price.  \n",
        "* For feature group 4, somewhat surprisingly the garage related features are all moderately correlated to sale price; also unexpected is the moderate correlation with sale price for the 'MasVnrArea' and 'OpenPorchSF' features. The remaining feature correlation strengths are as predicted.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phi_k correlation test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import phik\n",
        "from phik.phik import phik_matrix\n",
        "from phik.report import plot_correlation_matrix\n",
        "\n",
        "phik_matrix_df = pd.DataFrame(phik.phik_matrix(house_prices_df)['SalePrice'].sort_values())\n",
        "matrix_plot = plot_correlation_matrix(phik_matrix_df.values, x_labels=phik_matrix_df.columns, y_labels=phik_matrix_df.index, figsize=(10,10), vmin=0, vmax=1,\n",
        "                                      y_label='Feature', title='SalePrice-Feature pair $\\phi_k$ correlations ')\n",
        "matrix_plot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the phi_k matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    phik_matrix_df.to_csv(os.path.join(path, 'phik_matrix_df.csv'), index=True)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Significance**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phik.significance import significance_matrix\n",
        "significance_matrix_df = significance_matrix(house_prices_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the significance matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    significance_matrix_df.to_csv(os.path.join(path, 'phik_significance_matrix_df.csv'), index=True)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "plot_correlation_matrix(pd.DataFrame(significance_matrix_df['SalePrice']).sort_values(by='SalePrice').values,\n",
        "                                     x_labels=pd.DataFrame(significance_matrix_df['SalePrice']).sort_values(by='SalePrice').columns,\n",
        "                                     y_labels=pd.DataFrame(significance_matrix_df).sort_values(by='SalePrice').index, y_label='Feature',\n",
        "                                     title='Significance of $\\phi_k$ coefficients', vmin=0, vmax=5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The phi_k coefficient significance values for all SalePrice-feature pairs suggest the phi_k coefficients are statistically significant, with nearly all features having significance values greater than 5 standard deviations. The remaining features have values greater than 3 SD's."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With regard to the coefficients themselves, generally features that have moderate-to-strong spearman correlations, also seem to exhibit a similar strength (with some variation) dependence on sale price, as would be expected. However the '2ndFlrSF' feature exhibits a strong variable dependence with the sale price, despite having a weak spearman (r) value. Additionally the 'MasVnrArea' has a stronger dependence on sale price than what might be expected from its r-value; this could be a consequence of non-monotonic relationships, or the impact of outliers, but, an examination of the scatter plots may reveal this."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scatter plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating copy with ordinal encoding reversed\n",
        "house_prices_df_copy = house_prices_df.copy(deep=True)\n",
        "house_prices_df_copy[house_prices_categorical_df.columns] = encoder.inverse_transform(X=house_prices_df[house_prices_categorical_df.columns])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving this unencoded dataframe for use on the dashboard app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    house_prices_df_copy.to_csv(os.path.join(path, 'scatterplot_data_df.csv'), index=True)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_names = house_prices_df.columns.tolist()[0:-1]\n",
        "partitioned_names = []\n",
        "counter = 0\n",
        "no_of_groups = int(len(column_names)/4)\n",
        "while counter < no_of_groups:\n",
        "    partitioned_names.append(column_names[counter*4:counter*4 + 4])\n",
        "    counter +=1\n",
        "partitioned_names.append(column_names[-3:])\n",
        "\n",
        "for group in partitioned_names:\n",
        "    fig, axes = plt.subplots(ncols=len(group), nrows=1, figsize=(20,5), tight_layout=True)\n",
        "    for feature in group:\n",
        "        order = []\n",
        "        if feature == 'BsmtExposure':\n",
        "            order = bsmt_exposure_cat\n",
        "        elif feature == 'BsmtFinType1':\n",
        "            order = bsmt_fin_type1_cat\n",
        "        elif feature == 'GarageFinish':\n",
        "            order = garage_finish_cat\n",
        "        elif feature == 'KitchenQual':\n",
        "            order = kitchen_quality_cat\n",
        "\n",
        "        if len(order) == 0:\n",
        "            sns.scatterplot(data=house_prices_df_copy[[feature, 'SalePrice']], x=feature, y='SalePrice', ax=axes[group.index(feature)])\n",
        "        else:\n",
        "            sns.stripplot(data=house_prices_df_copy[[feature, 'SalePrice']], x=feature, y='SalePrice', ax=axes[group.index(feature)], order=order)\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The scatter-type plots largely agree with the calculated spearman coefficients.  \n",
        "\n",
        "* The r-values suggesting strong correlations are supported for the corresponding feature plots.  \n",
        "\n",
        "* Likewise for the moderate r-value features, it can be seen that whilst there are matching observable trends, there is a greater degree of clustering about certain values of a given feature, and where the variation is greater w.r.t sale price, and so this disrupts any potential monotonic trend.  \n",
        "\n",
        "*  Again the same is true for weak r-value features, where there is less of a clear monotonic trend, but instead either a static clustering, peaks that fall off, or no real pattern at all. For example for the feature 'OverallCond' there is a somewhat positive monotonic trend, that is counteracted by a clustering of values with the score 5 displaying a greater variation in sale price. A further example is for '2ndFlrSF', where if you were to ignore the zero values, a moderate monotonic trend would likely exist. A final illustrative case is for the feature 'BedroomAbvGr' where on average the sale price appears to increase with the number of bedrooms, peaking at 5, before dropping off --- this may be because the fewer values >5 are outliers.\n",
        "\n",
        "* The phi_k coefficients are also largely supported by the plots. Of course as these coefficients capture non-montonic relationships as well, you would expect them to suggest a stronger relationship for the features where there are peaks and or troughs in the plot. The large difference in magnitude of the spearman and phi_k coefficients for the feature 'OverallCond' can be understood from its aforementioned plot and the cluster of values with score 5 displaying greater sale price variation than for other scores. Similarly for '2ndFlrSF' the very large difference in coefficients can be understood by the cluster with value zero having a greater degree of sale price variation.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictive Power Score (PPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ppscore as pps\n",
        "pps_df = pps.predictors(df=house_prices_df, y='SalePrice', sample=1460, cross_validation=10)\n",
        "pps_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pps_heatmap = sns.heatmap(pps_df.pivot(index='x', columns=['y'], values=['ppscore']).sort_values(by=('ppscore', 'SalePrice'), ascending=False), annot=True,\n",
        "                                              vmax=1, vmin=0, xticklabels=['SalePrice'], linecolor='white', linewidth=0.05)\n",
        "pps_heatmap.set(xlabel='', ylabel='Feature', title='Feature PPS for target SalePrice')\n",
        "pps_heatmap"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* None of the PPS scores are above 0.5, however this is not surprising since no one feature in this dataset (many features exist) would be expected to account for all variation in the target.\n",
        "Instead a combination of the most significant features is likely to be able to predict the target more effectively. With this is mind any features with a PPS similar to the\n",
        "feature with the highest PPS, that also demonstrates a strong correlation, is likely to act as a benchmark for a 'good' score.\n",
        "\n",
        "* Thus the feature 'OverallQual' with a PPS of 0.44, that also has a strong correlation coefficients can act as a benchmark score.\n",
        "\n",
        "* The ranking of the features PPS's (grouped by strength) roughly concord with the ranking of their correlation coefficients, albeit with small differences.\n",
        "\n",
        "* However, the ratio of the 'GrLivArea' PPS to the 'OverallQual' PPS is larger than the respective ratios for the correlation coefficients. Also Features such as 'EnclosedPorchSF', and 'WoodDeckSF' have, relative to features with far stronger correlations, stronger PPS's which is unexpected.\n",
        "\n",
        "* At the same time features such as '1stFlrSF' and 'TotalBsmtSF' which have at least moderate correlation coefficients, have zero or close to zero PPS's, which again is unexpected.\n",
        "\n",
        "* The overall low scores and unexpected scores for various features, again is probably more of a reflection on the model used to predict the target, and its poor performance\n",
        "relative to the naive model, as well as the aforementioned point that the predictive power of a group of features will be far better than any individual feature."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hypotheses**:  \n",
        "\n",
        "* The common alternative hypothesis (positive monotonic correlation) for all features can be accepted, except for the 'OverallCond' and 'EnclosedPorch' features.\n",
        "\n",
        "**With regard to the strength of any relationship to the sale price, as implied by the correlation tests**:  \n",
        "\n",
        "* The quality feature group features have a strong relationship to sale price.\n",
        "\n",
        "* Of the size group features, all but the '2ndFlrSF', 'BedroomAbvGr', 'BsmtUnfSF', 'BsmtFinSF1' have at least moderate Spearman correlation; whilst all but 'LotArea', 'LotFrontage', 'BedroomAbvGr' have at least a moderate dependence on sale price.\n",
        "\n",
        "* The garage related features have at least a moderate correlation/dependence with sale price.\n",
        "\n",
        "* On the whole the enclosed porch related feature has a weak relationship to the sale price, whilst the open porch feature may have a moderate relationship.\n",
        "\n",
        "* Age related features, and the 'MasVnrArea' feature have at least a moderate relationship to the sale price.\n",
        "\n",
        "* The 'OverallCond', 'BedroomAbvGr' and 'WoodDeckSF' features have a weak relationship to the sale price.\n",
        "\n",
        "**Scatter plots**:\n",
        "\n",
        "* The scatter plots largely agree with the relationships implied by the correlation tests.\n",
        "\n",
        "* They also to some extent explain why certain features appear to have a weak monotonic relationship, but at least a moderate dependence on the sale price: namely because of certain feature values having greater variations in sale price; likewise weak relationships are illustrated in the plots as clustering with less variation.\n",
        "\n",
        "**PPS**:\n",
        "\n",
        "* The PPS's for all features are not that strong or always consistent with strength of the relationships implied by the correlation coefficients, but this is more likely because \n",
        "multiple features are necessary to predict the sale price.\n",
        "\n",
        "* However some of the strongest correlated features, also have the largest PPS's.\n",
        "\n",
        "**Selection of the most significant features for business requirement 2 user story task EDA**:\n",
        "\n",
        "Based on all metrics used, the following features have been deemed significant enough for further exploratory data analysis:\n",
        "* '1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'GarageArea', 'GarageFinish', 'GarageYrBlt', 'GrLivArea', 'KitchenQual', 'LotArea', 'LotFrontage', 'MasVnrArea', 'OpenPorchSF', 'OverallQual', 'TotalBsmtSF', 'YearBuilt', 'YearRemodAdd'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "selected_significant_features = ['1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'GarageArea', 'GarageFinish',\n",
        "                                 'GarageYrBlt', 'GrLivArea', 'KitchenQual', 'LotArea', 'LotFrontage',\n",
        "                                 'MasVnrArea', 'OpenPorchSF', 'OverallQual', 'TotalBsmtSF', 'YearBuilt',\n",
        "                                 'YearRemodAdd']\n",
        "\n",
        "path = os.path.join(os.getcwd(), 'outputs/selected_significant_features.pkl')\n",
        "\n",
        "joblib.dump(selected_significant_features, path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
